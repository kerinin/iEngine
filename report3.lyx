#LyX 1.6.2 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble



\usepackage{amsfonts}
\end_preamble
\use_default_options false
\language english
\inputencoding auto
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\leftmargin 1.5in
\topmargin 1in
\rightmargin 1.5in
\bottommargin 1.25in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\author "" 
\end_header

\begin_body

\begin_layout Title
Motivated Decision-Making using Transformation Invariant Probability Estimates
\end_layout

\begin_layout Author
Ryan Michael
\begin_inset Newline newline
\end_inset

 
\family typewriter
kerinin@gmail.com
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Subsection
Existing Work
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Expectation-maximization_algorithm
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Stationary_process
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Ergodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Mixing_(mathematics)
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Lyapunov_exponent
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_quantification_analysis
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Recurrence_plot
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autoregressive_fractionally_integrated_moving_averag
e
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%http://en.wikipedia.org/wiki/Autocorrelation
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
url{http://en.wikipedia.org/wiki/Linear_discriminant_analysis}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Hidden Markov Model
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% cannot account for future states - only capable of prediction
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% weak, short-term memory
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Box-Jenkins
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% intended for simplistic processes with well-understood stationarity and
 periodicity
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% http://en.wikipedia.org/wiki/Box-Jenkins
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Spectral Analysis
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% assumes some type of frequency-domain decomposition.
  Frequency-domain signal representations do not do a very good job predicting
 time-domain values.
 
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Shrinking-
\begin_inset Formula $\epsilon$
\end_inset

 SVM Regression
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% theoretical foundation weak; only compensates for the relevance of recent
 data.
  See Markhov problem
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
General Overview
\end_layout

\begin_layout Standard
The goal is to create a method of statistical inference capable of processing
 multiple sources of data which are both multi-variate and exhibit transformatio
n-invariant behaviors.
 This type of data is common, and developing a robust method of analysis
 has applications in many domains.
 
\end_layout

\begin_layout Standard
For any estimator of a probability density function, the accuracy (measured
 by the confidence interval) of the estimator can be shown to depend on
 two factors; the number of observations from which the estimates are made
 and the amount of information the estimator is capable of extracting from
 from those observations.
 Existing estimators, such as the Parzen Window estimator extract information
 about the probability of a given point by comparing the distance between
 that point and the set of points previously observed; the greater the number
 of points in that are 'close', the higher the probability.
 It is possible to determine the confidence interval of such an estimator
 using the VC Entropy of the estimating function.
 The VC Entropy of an estimator is based on the number of points which are
 needed to 'cover' the observations with some level of closeness.
 This is to say that if the set of observations contain multiple points
 which are 'close' to each other, the estimator can ignore all but one in
 generating estimates (provided the number of similar points is retained).
\end_layout

\begin_layout Standard
Our task is to develop a set of estimators whose confidence interval is
 less than the confidence interval determined by the VC Entropy, and we
 do so by extending the conceprt of VC Entropy to sets of observations under
 linear transformations.
 Where the VC Entropy is determined by the number of points necessary to
 'cover' the set of observations, we define the Transformation-Invariant
 Entropy (TI Entropy) as the number of 
\emph on
sets of points
\emph default
 necessary to 'cover' the observations under linear transformations.
 In the same way that the VC Entropy is determined by the similarity between
 individual observations, the TI Entropy is determined between sets of observati
ons.
 
\end_layout

\begin_layout Standard
The distance between two points can easily be determined using well-known
 distance metrics; the distance between two sets of points requires that
 we develop a new distance metric.
 This distance metric measures the potential similarity between two sets
 when one has been subjected to some linear transformation.
 For simplicity we will integrate this similarity measure over all possible
 transformations; the resulting similarity measure will describe the net
 similarity of the two sets under 
\emph on
any
\emph default
 linear transformation.
 It is for this reason that we refer to our extension of the VC Entropy
 as Transformation-Invariant.
 It will be shown that the TI Entropy is necessarily less than or equal
 to the VC Entropy, and that as a result the confidence interval of an estimator
 based on the TI-distance between sets of observations is necessarily greater
 than or equal to that of estimators based on the distance between individual
 points.
 
\end_layout

\begin_layout Standard
Because the computational complexity of TI-based estimators is higher than
 that of traditional estimators we will develop search heuristics allowing
 us to confine the TI analysis to subsets of the observations which are
 likely to provide useful results.
 We will develop an ensemble system capable of applying TI analysis to multiple
 subsets and combining the results of these analyses.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Intro motivation and decision-making
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Formal Setting
\end_layout

\begin_layout Standard
We use the notation of Vapnik - given three components:
\end_layout

\begin_layout Enumerate
A generator (G) of random vectors 
\begin_inset Formula $x\in\mathbb{R}^{n}$
\end_inset

, drawn independantly from a fixed but unknown probability distribution
 function 
\begin_inset Formula $F(x)$
\end_inset


\end_layout

\begin_layout Enumerate
A supervisor (S) who returns an output value 
\begin_inset Formula $y$
\end_inset

 to every input vector 
\begin_inset Formula $x$
\end_inset

, according to a conditional distribution function 
\begin_inset Formula $F(y|x)$
\end_inset

, also fixed but unknown
\end_layout

\begin_layout Enumerate
A learning machine (LM) capable of implementing a set of functions 
\begin_inset Formula $\varphi(x,\alpha),\ \alpha\in\Lambda$
\end_inset

, where 
\begin_inset Formula $\Lambda$
\end_inset

 is a set of parameters
\end_layout

\begin_layout Standard
Our goal is to choose the function which best approximates the supervisor's
 response, given a set of observations:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
X & =x_{1},...,x_{\ell}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We choose between potential functions based on the empirical risk function
 using the loss (discrepancy) 
\begin_inset Formula $L(y,f(x,\alpha))$
\end_inset

 between the response 
\begin_inset Formula $y$
\end_inset

 for a given value of 
\begin_inset Formula $x$
\end_inset

 and the predicted value 
\begin_inset Formula $f(x,\alpha)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
R_{\text{emp}}(\alpha) & =\frac{1}{\ell}\sum_{i=1}^{\ell}L(y_{i},f(x_{i},\alpha))\\
 & =\frac{1}{\ell}\sum_{i=1}^{\ell}Q(z_{i},\alpha)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We define our problem as one of estimating a Probability Density Functions
 (PDF), so we define the set of observations as a random variable of arbitrary
 dimensions:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
X & =(\Omega,\mathcal{F},\mathcal{P})\\
\Omega & \in\mathbb{R}^{d}\\
X & =[\vec{x}_{1},...,\vec{x}_{\ell}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Our objective is to develop an estimate 
\begin_inset Formula $\varphi(x:\ X)$
\end_inset

 of the probability of the given point 
\begin_inset Formula $x$
\end_inset

 given a set of observations 
\begin_inset Formula $X$
\end_inset

 assuming some minimal uncertainty 
\begin_inset Formula $\xi$
\end_inset

 in the data:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ X) & \longmapsto\Pr(x|X)\pm\xi\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Organize as follows: 1)Introduction and formal setting 2) Distance measure
 3) Parzen Estimator and SV Estimator 4) TI Entropy measure 5) Ensemble
 System 6) Motivated Decision Making
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Reorganize paper: 1) definition of similarity between subsets 2) probability
 estimation over subsets 3) algorithm for choosing subsets, A1) SVM, A2)
 ensemble system
\end_layout

\end_inset


\end_layout

\begin_layout Section
Transformation Invariant Parzen Windows
\end_layout

\begin_layout Subsection
Parzen Windows for i.i.d.
 Data
\end_layout

\begin_layout Standard
Our task is to estimate the probability of a given vector 
\begin_inset Formula $\vec{x}$
\end_inset

 in the abstract space 
\begin_inset Formula $\Omega$
\end_inset

 based on a set of observations 
\begin_inset Formula $X$
\end_inset

.
 One method of accomplishing this is by using the Parzen Window (PW) method.
 We choose the Parzen Window method because it allows us to estimate probabiliti
es of unordered sets, provided they have an addition operation and a kernel
 function exists to provide a distance metric.
 The basic operation of the PW method is to estimate the probability of
 a point based on the sum of the distance from that point to each point
 in a set of prior observations.
 The Parzen Window approach to probability density function (PDF) estimation
 is as follows; given a set of prior observations 
\begin_inset Formula $X$
\end_inset

 and a kernel function 
\begin_inset Formula $K_{\gamma}(\cdot,\cdot)$
\end_inset

 with width parameter 
\begin_inset Formula $\gamma$
\end_inset

, the probability of a point 
\begin_inset Formula $\vec{x}$
\end_inset

 is determined by :
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(\vec{x}:\ X) & =\sum_{i}^{\ell}\frac{1}{\ell}K_{\gamma}(\vec{x},\vec{x}_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Multiple kernel functions exist, in this paper we will use the Radial Basis
 Function (RBF) kernel:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
K_{\gamma}\left(\vec{x},\vec{y}\right) & =\prod_{\upsilon=1}^{d}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|x^{\nu},y^{\nu}\|^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 is some metric, for instance the 
\begin_inset Formula $L^{2}$
\end_inset

 distance:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|x,y\| & =\left(x-y\right)^{2}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This approach is useful in the context of a set of observations drawn i.i.d.
 from a static PDF, however it is unable to address transformation invariance.
 Transformation invariance (TI) refers to datasets in which one or multiple
 PDF's occur in various transformed states within the dataset, for example
 a given sound (described by a known PDF) could be repeated multiple times
 in an audio recording, or a given image could appear in multiple locations
 in a larger image.
 While the PW method can generate an estimate in the presence of data generated
 by transformed PDF's, its inability to recognize multiple instances of
 a single PDF limits its rate of convergence.
 
\end_layout

\begin_layout Subsection
Contextual Estimation
\end_layout

\begin_layout Standard
The crucial distinction between i.i.d data from a static PDF and data which
 exhibits TI is that when estimating the probability of a point, you must
 consider to context of the point as well as the location of the point in
 
\begin_inset Formula $\Omega$
\end_inset

.
 In this paper we will establish the context of 
\begin_inset Formula $x$
\end_inset

 by defining a 'neighborhood' of points around 
\begin_inset Formula $x$
\end_inset

 which we will refer to as a window 
\begin_inset Formula $w$
\end_inset

 on 
\begin_inset Formula $X$
\end_inset

.
 Because a TI analysis may be applicable to some dimensions of 
\begin_inset Formula $\Omega$
\end_inset

 and not other, we will define the set of dimensions for which TI analysis
 is used as 
\begin_inset Formula $D$
\end_inset

, the the set of dimensions 
\emph on
not 
\emph default
used for TI analysis as 
\begin_inset Formula $\tilde{D}$
\end_inset

, and the set of all dimensions of 
\begin_inset Formula $\Omega$
\end_inset

 as 
\begin_inset Formula $D^{X}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
D^{X} & =[0,...,d]\\
D & \subseteq D^{X}\\
\tilde{D} & =[n\in D^{X}|\ n\notin D]\\
\vec{x}_{i} & \longmapsto w_{i}^{D}\\
w_{i}^{D} & =[x_{i}^{n}|\ n\in D]\\
w_{i}^{n} & =x_{i}^{n}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
While the window 
\begin_inset Formula $w$
\end_inset

 describes the 
\emph on
location 
\emph default
of a window, we still must define its extents.
 We will do so through the use of a windowing kernel function 
\begin_inset Formula $\omega(\cdot)$
\end_inset

 parameterized by some variable 
\begin_inset Formula $\alpha$
\end_inset

 which returns a value describing the degree of inclusion of an arbitrary
 point and 
\begin_inset Formula $x$
\end_inset

 in the window 
\begin_inset Formula $w$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\omega_{\alpha}(w,x) & \Rightarrow[0,\infty)\\
\int_{-\infty}^{\infty}\omega_{\alpha}(w,x)dx_{i} & =1\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can use any kernel function for 
\begin_inset Formula $\omega(\cdot)$
\end_inset

, however we assume that the windowing function has a peak at 
\begin_inset Formula $w_{i}=[x_{j}^{n}|\ n\in D]$
\end_inset

 and that it drops off as 
\begin_inset Formula $\left\Vert w_{i}-[x_{j}^{n}|\ n\in D]\right\Vert \rightarrow\pm\infty$
\end_inset

.
 Notice that we use a real-valued rather than boolean inclusion metric -
 this allows us to 'smear' the context of a given window into adjacent areas
 of 
\begin_inset Formula $X^{D}$
\end_inset

.
\end_layout

\begin_layout Subsection
Parzen Windows for Transformation-Invariant Data
\end_layout

\begin_layout Standard
Because we must establish a context for TI data, we must think of the set
 of observations 
\begin_inset Formula $X$
\end_inset

 as part of both the problem definition and the solution.
 This means that we can no longer simply calculate the kernel distance between
 
\begin_inset Formula $x$
\end_inset

 and each of the observations in 
\begin_inset Formula $X$
\end_inset

 independantly - we must compare the 
\emph on
context
\emph default
 of 
\begin_inset Formula $x$
\end_inset

 with contexts of 
\begin_inset Formula $X$
\end_inset

.
 This requires a kernel function capable of comparing two 
\emph on
sets of points
\emph default
.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \begin{align*}
K_{\gamma}\left(X_{i},X_{j}\right) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|X_{i},X_{j}\|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can simplify this equation by observing that we are not actually dealing
 with two sets; we're dealing with two windows 
\begin_inset Formula $w_{n}$
\end_inset

 and 
\begin_inset Formula $w_{m}$
\end_inset

 within 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \begin{align*}
K_{\gamma}\left(w_{n},w_{m},X\right) & =\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|w_{n},w_{n},X\|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can extend this basic result to multiple dimensions by using the tensor
 product of the kernel values.
 In this context we must distinguish between TI dimensions and non-TI dimensions
, as the distance metric will be different for e
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
ach
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
:
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset Formula \begin{align*}
K_{\gamma}\left(w_{n}^{D},w_{m}^{D},X\right) & =\prod_{\nu\in\tilde{D}}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\left(x_{n}^{\nu}-x_{m}^{\nu}\right)^{2}}\prod_{\nu\in D}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|w_{n}^{D},w_{n}^{D},\nu,X\|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
There are multiple divergence measures available which can be used to define
 a 'distance' between two sets.
 We will later show the importance of the distance metric 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 being integrable; for this reason we will use the Pearson Divergence as
 our distance metric when comparing sets.
 In order to acommodate linear transformations, we add a 
\begin_inset Formula $d\times d$
\end_inset

 transformation matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 and a 
\begin_inset Formula $d\times1$
\end_inset

 shift matrix 
\begin_inset Formula $\mathbf{b}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|X^{\nu},Y^{\nu}\| & =\sum_{x^{\nu}\in\{X\cup Y\}}\left(\frac{\varphi(x^{\nu}:\ \mathbf{A}^{\nu}X^{\nu}+\mathbf{b}_{\nu})}{\varphi(x^{\nu}:\ Y^{\nu})}-1\right)^{2}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because the Pearson Divergence is a summation, we can control the influence
 of each point by scaling it using the windowing function:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|w_{n}^{D},w_{m}^{D},\nu,X\| & =\sum_{i=1}^{\ell}\omega_{\alpha}(w_{n}^{\nu},x_{i}^{\nu})\left(\frac{\varphi(x_{i}^{\nu}:\ w_{m}^{D},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ w_{m}^{D},X)}-1\right)^{2}\\
\varphi(x^{\nu}:\ w_{n}^{D},X) & =\sum_{i=1}^{\ell}\frac{\omega_{\alpha}(w_{n}^{\nu},x_{i}^{\nu})}{\sum_{j=1}^{\ell}\omega_{\alpha}(w_{n}^{\nu},x_{j}^{\nu})}K_{\gamma}(x^{\nu},x_{i}^{\nu})\\
 & =\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(w_{n}^{\nu},x_{i}^{\nu},X)\ K_{\gamma}(x^{\nu},x_{i}^{\nu})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
These two matrix transformations are used to shift, scale, rotate, shear
 or mirror the window 
\begin_inset Formula $w_{n}^{D}$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

.
 The first transformation matrix 
\begin_inset Formula $\mathbf{A}$
\end_inset

 is an 
\begin_inset Formula $d\times d$
\end_inset

 matrix.
 The linear operator 
\begin_inset Formula $\mathbf{A}X$
\end_inset

 allows us to scale, rotate, shear, and mirror 
\begin_inset Formula $X$
\end_inset

 depending on the matrix values of 
\begin_inset Formula $\mathbf{A}$
\end_inset

.
 The second transformation amtrix 
\begin_inset Formula $\mathbf{b}$
\end_inset

 is a 
\begin_inset Formula $d\times1$
\end_inset

 matrix; adding these terms together allows us to shift 
\begin_inset Formula $X$
\end_inset

 along any axis based on the values of 
\begin_inset Formula $\mathbf{b}$
\end_inset

.
 
\end_layout

\begin_layout Standard
One approach to these transformations is to explicitly determine values
 for the two matrices and to determine the distance 
\begin_inset Formula $\|\cdot,\cdot\|$
\end_inset

 using these values.
 A more robust approach is to integrate over all possible values of 
\begin_inset Formula $(\mathbf{A},\mathbf{b}):$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|w_{n}^{D},w_{m}^{D},\nu,X\| & =\sum_{i=1}^{\ell}\omega_{\alpha}(w_{n}^{\nu},x_{i}^{\nu})\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x_{i}^{\nu}:\ w_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ w_{m}^{\nu},X^{\nu})}-1\right)^{2}d\mathbf{A}d\mathbf{b}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This approach allows us to compare the distance between 
\emph on
any
\emph default
 linear transformation 
\begin_inset Formula $\mathbf{A}X+\mathbf{b}$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

, a far more powerful and less computationally demanding approach.
 Doing so requires that we calculate the following integral:
\end_layout

\begin_layout Standard
\begin_inset Formula \[
\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x_{i}^{\nu}:\ w_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ w_{m}^{\nu},X^{\nu})}-1\right)^{2}d\mathbf{A}d\mathbf{b}\]

\end_inset


\end_layout

\begin_layout Standard
Because 
\begin_inset Formula $(\mathbf{A},\mathbf{b})$
\end_inset

 is a pair of matrix transformation, we must integrate the previous equation
 element-wise:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\end{align*}

\end_inset


\begin_inset Formula \begin{align*}
\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x_{i}^{\nu}:\ w_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ w_{m}^{\nu},X^{\nu})}-1\right)^{2}d\mathbf{A}d\mathbf{b} & =\int...\int_{-\infty}^{\infty}\left(\frac{\varphi(x_{i}^{\nu}:\ w_{m}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x_{i}^{\nu}:\ w_{m}^{\nu},X^{\nu})}-1\right)^{2}d\mathbf{A}_{\nu,0},...,d\mathbf{A}_{\nu,d}d\mathbf{b}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Explain all this shit below
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
f([x^{0},...,x^{d}]:\ a,b,c,f,h) & =\left(a\ \exp\left(-b\left(\left(x^{d}\right)^{2}+x^{d}\sum_{i=1}^{d-1}x^{i}c^{i}+f\right)\right)-h\right)^{2}\\
\int...\int f([x^{0},...,x^{d}]:\ a,b,c,f,h)\ dx^{0},...,dx^{d} & =\int...\int f\left([x^{0},...,x^{d-1}]:\ \frac{\sqrt{\frac{\pi}{2}}a^{2}}{\sqrt{b}},-\frac{b}{2},-2bf,0\right)\ dx^{0},...,dx^{d-1}\\
 & \qquad\qquad+\int f\left([x^{0},...,x^{d-1}]:\ \frac{-2a\sqrt{\pi}}{\sqrt{b}},-\frac{b}{4},-4f,0\right)\ dx^{0},...,dx^{d-1}\\
\int f([x^{0}]:\ a,b,f,h)\ dx^{0} & =\frac{\sqrt{\frac{\pi}{2}}a^{2}}{\sqrt{b}}\exp\left(-\frac{1}{2}b^{2}f\right)-\frac{2a\sqrt{\pi}}{\sqrt{b}}\exp\left(-bf\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\int\int_{-\infty}^{\infty}\left(\frac{\varphi(x^{\nu}:\ w_{n}^{\nu},\mathbf{A}X+\mathbf{b})}{\varphi(x^{\nu}:\ w_{n}^{\nu},X)}-1\right)^{2}d\mathbf{A}d\mathbf{b} & =\int...\int\left(\frac{\varphi\left(x^{\nu}:\ w_{n}^{\nu},\left[\mathbf{b}_{\nu}+\sum_{n=1}^{|D|}\mathbf{A}_{\nu,n}x_{i}^{n}|\ x_{i}\in X\right]\right)}{\varphi(x^{\nu}:\ w_{n}^{\nu},X)}-1\right)^{2}d\mathbf{A}d\mathbf{b}\\
 & =\int...\int f([x^{0},...,x^{d}]:\ a,b,c,f,h)\ dx^{0},...,dx^{d}d\mathbf{b}_{\nu}\\
a & =\frac{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(w_{n}^{\nu},x_{i}^{\nu},X)}{\sum_{i=1}^{\ell}\ddot{\omega}_{\alpha}(w_{n}^{\nu},x_{i}^{\nu},X)\ \exp\left(-\frac{1}{\gamma}\left(x^{\nu}-x_{i}^{\nu}\right)^{2}\right)}\\
b & =\frac{1}{\gamma}\\
c & =???\\
f & =\left(x^{\nu}\right)^{2}-2x^{\nu}\mathbf{b}_{\nu}+\mathbf{b}_{\nu}^{2}\\
h & =1\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Make a note that for d not in D, A_dd=0, b=0
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Performance
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Big-O
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Multiple Data Sources
\end_layout

\begin_layout Standard
Consider the case where data from multiple data sources contributes to our
 set of observations 
\begin_inset Formula $X$
\end_inset

, for instance the data drawn from a microphone and a video camera.
 Let us assume that both input sources are timestamped, but the sampling
 frequency is different for each source and that the vector data points
 have different dimensionality.
 We will refer to observations of the former as 
\begin_inset Formula $X=(\Omega^{X},\mathcal{F}^{X},\mathcal{P}^{X})$
\end_inset

 and the latter as 
\begin_inset Formula $Y=(\Omega^{Y},\mathcal{P}^{Y},\mathcal{F}^{Y})$
\end_inset

.
 In this situation we can treat the timestamp as a 'shared' dimension, but
 all other dimensions of the two vectors are independant.
 It is clear that if we intend to establish a PDF of the joint probability
 space 
\begin_inset Formula $(\Omega^{Y,X},\mathcal{P}^{Y,X},\mathcal{F}^{Y,X})$
\end_inset

, we must treat each dimension in 
\begin_inset Formula $\Omega^{X}$
\end_inset

 and 
\begin_inset Formula $\Omega^{Y}$
\end_inset

 as orthonormal to each other.
 The fact that both event spaces share a time dimension means that a TI
 analysis over the two time dimensions is likely to produce useful results.
 
\end_layout

\begin_layout Standard
The most straightforward way to handle this situation is to assume that
 vector observations constitute sparse matrices; for any given dimension
 of an observation 
\begin_inset Formula $\vec{x}$
\end_inset

, the value can either be a real number or null:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
x^{\nu} & \in[\mathbb{R},\textrm{Ã˜}]\end{align*}

\end_inset

 In this case we only include operations between real-valued dimensions
 in our analysis.
 We can easily accomodate the shared time dimension by including the two
 time dimensions in our TI analysis.
 This allows us to consider not only the situation where both inputs are
 timestamped with accurate clocks, but the situation where the two clocks
 are independantly inaccurate and some transformation is required for them
 to show the same time.
\end_layout

\begin_layout Subsection
Single Channel Summary
\end_layout

\begin_layout Standard
We have developed a PDF estimation technique which allows us to take advantage
 of TI data.
 The solution uses a novel distance metric to compare the similarity between
 two sets of points in the context of arbitrary linear transformations of
 one set.
 The solution proposed is restricted to observations with shared dimensionality,
 however it imposes no restrictions on which dimensions are TI.
 The types of transformations considered by the proposed solution are restricted
 to linear matrix transformations shifts.
 In the next section we will extend the solution to cases where multiple
 'channels' of data are present allowing us to handle TI between observations
 with non-shared dimensions.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(\vec{x}:\ X) & =\sum_{i=1}^{\ell}\frac{1}{\ell}\ K_{\gamma}(w^{D},w_{i}^{D},\{\vec{x}\cup X\})\\
K_{\gamma}\left(w_{n}^{D},w_{m}^{D},X\right) & =\prod_{\nu\in\tilde{D}}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\left(x_{n}^{\nu}-x_{m}^{\nu}\right)^{2}}\prod_{\nu\in D}\frac{1}{\gamma\sqrt{2\pi}}e^{-\frac{1}{\gamma}\|w_{n}^{D},w_{n}^{D},\nu,X\|}\\
\|w_{n}^{D},w_{m}^{D},\nu,X\| & =???\\
\ddot{\omega}_{\alpha}(w_{n}^{\nu},x_{i}^{\nu},X) & =\frac{\omega_{\alpha}(w_{n}^{\nu},x_{i}^{\nu})}{\sum_{j=1}^{\ell}\omega_{\alpha}(w_{n}^{\nu},x_{j}^{\nu})}\end{align*}

\end_inset


\end_layout

\begin_layout Section
Support Vector Optimizations
\end_layout

\begin_layout Standard
The Parzen Window method is neither sparse nor computationally efficient,
 and as the number of observations grows, these deficiencies quickly become
 prohibitive.
 We now investigate the use of Support Vector Machines to generate estimates.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Refer to big-O
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Generalized Parzen-SVM 
\end_layout

\begin_layout Standard
Support Vector Machines (SVM) are often used
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
ref?
\end_layout

\end_inset

 to estimate probability distributions by solving the related problem of
 estimating the cumulative distribution function of the random variable
 in question.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This isn't really the essence of SVM - put more verbage in the intro regarding
 what SVM's are, why they work, and why they're superior to other approaches
 (say, neural networks)
\end_layout

\end_inset

 This reduces the problem to one of estimating a non-linear mapping from
 observations to cumulative distribution values, which can be formulated
 as an optimization problem over a linear operator equation.
 Unfortunately, these methods depend on the ability to calculate an empirical
 distribution for each observation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
F_{\ell}(x)=\frac{1}{\ell}\sum_{i}\theta(x-x_{i})\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\theta(x)$
\end_inset

 is the indicator function.
 To evaluate this function, the abstract space 
\begin_inset Formula $\Omega$
\end_inset

 must be ordered.
 While we have described a distance metric over 
\begin_inset Formula $\Omega$
\end_inset

, it is not clear what a meaningful ordering relation would be.
\end_layout

\begin_layout Standard
Rather than calculating the cumulative probability distribution of 
\begin_inset Formula $X$
\end_inset

, we begin with the assumption that the PW estimate of the probability distribut
ion is acceptably accurate and attempt to minimize the difference between
 the Support Vector (SV) estimate and the PW estimate.
 In this spirit, we will use a modification of the PW estimator which substitute
s a set of weights 
\begin_inset Formula $\beta$
\end_inset

 for the normalizing contant 
\begin_inset Formula $\frac{1}{\ell}$
\end_inset

 in the PW equation:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(\vec{x}:\ \beta,X) & =\sum_{i=1}^{\ell}\beta_{i}K_{\gamma}(w^{D},w_{i}^{D},X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The support vector approach requires that we define an optimization problem
 
\begin_inset Formula $W(\beta)$
\end_inset

 to determine the value of 
\begin_inset Formula $\beta$
\end_inset

.
 This optimization problem will determine which observations (or as we shall
 see, windows) will be used in estimations and which can be discarded as
 redundant or irrelevant information.
 The result of the optimization problem will be that a substantial number
 of multipliers 
\begin_inset Formula $\beta_{i}$
\end_inset

 will be 
\begin_inset Formula $0$
\end_inset

, allowing us to omit the windows defined by these points in the prediction
 phase.
 We define the optimization problem as minimizing the square loss between
 the SV and PW estimates over some set of observations 
\begin_inset Formula $X$
\end_inset

.
 The set of weights used in the Support Vector estimation must have a discrete
 number of elements; for simplicity we choose to assign a weight to each
 window defined by the time value of an observation in the training set
 and the constant parameter 
\begin_inset Formula $\alpha$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta:\ X) & \longmapsto\min_{\beta}\sum_{i=1}^{\ell}\left(\varphi(\vec{x}_{i}:\ X)-\varphi(\vec{x}_{i}:\ \beta,X)\right)^{2}+\beta\Omega(\lambda,X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Discuss regularizier
\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the optimization problem, we check the difference between the two estimates
 at windows defined by the observations 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Discuss the equations below
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\sum_{i=1}^{\ell}\left(\varphi(\vec{x}_{i}:\ X)-\varphi(\vec{x}_{i}:\ \beta,X)\right)^{2} & =\sum_{i=1}^{\ell}\left(\varphi(\vec{x}_{i}:\ X)\right)^{2}-2\left(\varphi(\vec{x}_{i}:\ X)\varphi(\vec{x}_{i}:\ \beta,X)\right)+\left(\varphi(\vec{x}_{i}:\ \beta,X)\right)^{2}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
 & =\sum_{i=1}^{\ell}\left(\sum_{j=1}^{\ell}\frac{1}{\ell}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)\right)^{2}-2\left(\sum_{j=1}^{\ell}\frac{1}{\ell}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)\right)\left(\sum_{j=1}^{\ell}\beta_{j}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)\right)\\
 & \qquad\qquad\qquad+\left(\sum_{j=1}^{\ell}\beta_{j}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)\right)^{2}\\
 & =\sum_{i=1}^{\ell}\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\frac{1}{\ell^{2}}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)K_{\gamma}(w_{i}^{D},w_{k}^{D},X)-2\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\frac{1}{\ell}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)\beta_{j}K_{\gamma}(w_{i}^{D},w_{k}^{D},X)\\
 & \qquad\qquad\qquad+\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\beta_{j}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)K_{\gamma}(w_{i}^{D},w_{k}^{D},X)\\
 & =\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\beta_{j}\beta_{k}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)K_{\gamma}(w_{i}^{D},w_{k}^{D},X)-\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\frac{2\beta_{j}}{\ell}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)K_{\gamma}(w_{i}^{D},w_{jk}^{D},X)\\
 & \qquad\qquad\qquad+\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\frac{1}{\ell^{2}}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)K_{\gamma}(w_{i}^{D},w_{k}^{D},X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Because this is a minimization problem we can eliminate the last term (changing
 
\begin_inset Formula $\beta$
\end_inset

 won't affect its value).
 Substituting our optimization problem becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\beta:\ X) & \longmapsto\min_{\beta}\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\beta_{j}\beta_{k}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)K_{\gamma}(w_{i}^{D},w_{k}^{D},X)\\
 & \qquad\qquad\qquad-\sum_{\begin{array}{c}
i=1\\
j=1\\
k=1\end{array}}^{\ell}\frac{2\beta_{j}}{\ell}K_{\gamma}(w_{i}^{D},w_{j}^{D},X)K_{\gamma}(w_{i}^{D},w_{k}^{D},X)+\lambda\Omega(\beta,X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\text{subject to} & \beta_{i}\ge0,\ \sum\beta=1\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Shouldn't the second term only have one kernel function? If not combine
 the first two terms
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Quadratic Optimization Problem
\end_layout

\begin_layout Subsection
Support Vector Decomposition 
\end_layout

\begin_layout Standard
\begin_inset Marginal
status open

\begin_layout Plain Layout
This section is probably better as an appendix
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Due to the simplicity of the constraints in the optimization problem, it
 is possible to use the decomposition method of Osuna to reduce the memory
 requirements of the Parzen-SVM algorithm.
\end_layout

\begin_layout Subsubsection
 Sub-Problem Definition 
\end_layout

\begin_layout Standard
The decomposition algorithm breaks 
\begin_inset Formula $X$
\end_inset

 into two working sets 
\begin_inset Formula $B,N$
\end_inset

, and attempts to optimize 
\begin_inset Formula $B$
\end_inset

 while keeping 
\begin_inset Formula $N$
\end_inset

 fixed.
 This results in the following iterative optimization problem where 
\begin_inset Formula $\boldsymbol{\beta}^{k}$
\end_inset

 denotes the result of the previous iteration:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
W(\boldsymbol{\beta}_{B}) & =\frac{1}{2}\begin{bmatrix}\boldsymbol{\beta}_{B}^{T} & (\boldsymbol{\beta}_{N}^{k})^{T}\end{bmatrix}\begin{bmatrix}P_{BB} & P_{BN}\\
P_{NB} & P_{NN}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}-\begin{bmatrix}q_{B}^{T} & q_{N}^{T}\end{bmatrix}\begin{bmatrix}\boldsymbol{\beta}_{B}\\
\boldsymbol{\beta}_{N}^{k}\end{bmatrix}\\
 & =\frac{1}{2}\boldsymbol{\beta}_{B}^{T}P_{BB}\boldsymbol{\beta}_{B}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\boldsymbol{\beta}_{B}\\
 & =\frac{1}{2}\begin{bmatrix}\beta_{i} & \beta_{j}\end{bmatrix}\begin{bmatrix}P_{ii} & P_{ij}\\
P_{ij} & P_{jj}\end{bmatrix}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}-(-q_{B}+P_{BN}\boldsymbol{\beta}_{N}^{k})^{T}\begin{bmatrix}\beta_{i}\\
\beta_{j}\end{bmatrix}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\text{subject to}\quad0\le\beta_{i},\beta_{j},\quad\beta_{i}+\beta_{j}=1-\mathbf{1}^{T}\boldsymbol{\beta}_{N}^{k}\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
 Working Set Selection 
\end_layout

\begin_layout Standard
Select
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
 & i\in\text{arg}\max_{t}\left\{ -\nabla f(\boldsymbol{\beta}^{k})_{t}\ |\quad t\in I(\boldsymbol{\beta}^{k})\right\} \\
 & j\in\text{arg}\min_{t}\left\{ -\frac{b_{it}^{2}}{a_{it}}\ |\quad t\in I(\boldsymbol{\beta}^{k}),\quad-\nabla f(\boldsymbol{\beta}^{k})_{t}<-\nabla f(\boldsymbol{\beta}^{k})_{i}\right\} \end{align}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% This needs to be checked for the new optimization scenario
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align}
I(\boldsymbol{\beta}) & \equiv\{t\ |\quad\beta_{t}<1\quad\text{or}\quad\beta_{t}>0\}\\
 & a_{it}=P_{ii}+P_{tt}-2P_{it}\\
 & \bar{a}_{it}=\begin{cases}
a_{it} & \text{if}\ a_{it}>0\\
\delta & \text{otherwise}\end{cases}\\
 & b_{it}=-\nabla f(\boldsymbol{\beta}^{k})_{i}+\nabla f(\boldsymbol{\beta}^{k})_{t}\\
\nabla f(\boldsymbol{\beta})_{i} & \equiv P_{i}\boldsymbol{\beta}-q_{i}\end{align}

\end_inset


\end_layout

\begin_layout Subsubsection
 Stopping Condition 
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{equation}
\max_{i\in I(\boldsymbol{\alpha}^{k})}-\nabla f(\boldsymbol{\alpha})_{i}+\min_{j\in I(\boldsymbol{\alpha}^{k})}\nabla f(\boldsymbol{\alpha})_{j}\le\epsilon\end{equation}

\end_inset


\end_layout

\begin_layout Section
Ensemble System
\end_layout

\begin_layout Standard
\begin_inset Marginal
status open

\begin_layout Plain Layout
This section is likely to change drastically
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We now discuss creating a system composed of multiple estimators.
 It is well known that using a combination of estimates drawn from different
 models of an underlying phenomenon tends to increase the prediction accuracy
 of the hybrid system.
 We will begin by establishing an explanation for this phenomenon based
 on the concepts of VC Entropy and the uniform bounds on convergence of
 learning processes.
 We will then develop a method of qantifying the rate of convergence of
 an individual estimator, and show how this quantification can be used to
 build an optimal ensemble system of estimators.
 The basic question we will address is 
\emph on
how does one choose sets of dimension which will benefit from TI analysis,
 and how do we control the computational demands of an ensemble system?
\end_layout

\begin_layout Subsection
VC Entropy and Bounds on the Rate of Convergence
\end_layout

\begin_layout Standard
We begin by describing the VC entropy of a set of estimators using their
 risk values 
\begin_inset Formula $Q(x:\ \alpha)$
\end_inset

 defined by the set of parameters 
\begin_inset Formula $\alpha$
\end_inset

.
 Given a set of observations 
\begin_inset Formula $X=[x_{1},...,x_{\ell}]$
\end_inset

, we can construction a set of 
\begin_inset Formula $\ell$
\end_inset

-dimensional vectors
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
q(\alpha) & =[Q(x_{1},\alpha),...,Q(x_{\ell},\alpha)]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Each vector describes the loss of a given element of 
\begin_inset Formula $\alpha$
\end_inset

 for the observations 
\begin_inset Formula $X$
\end_inset

.
 We define the minimum number of vectors required to 'cover' 
\begin_inset Formula $q(\alpha)$
\end_inset

 with some arbitrarily small measure of closeness 
\begin_inset Formula $\varepsilon$
\end_inset

 as 
\begin_inset Formula $N^{\Lambda}(\varepsilon:x_{1},...,x_{\ell})$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
For example, if 
\begin_inset Formula $q(\alpha)=[1,2,4,1]$
\end_inset

, 
\begin_inset Formula $N^{\Lambda}(X)=3$
\end_inset


\end_layout

\end_inset

.
 Using this value, we caluclate the VC Entropy 
\begin_inset Formula $H^{\Lambda}(X)$
\end_inset

 of the set of functions 
\begin_inset Formula $Q(x,\alpha)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H^{\Lambda}(\varepsilon:\ell) & =\ln N^{\Lambda}(\varepsilon:x_{1},...,x_{\ell})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The VC Entropy essentially describes the number of observations in 
\begin_inset Formula $X$
\end_inset

 required to describe the PDF 
\begin_inset Formula $\mathcal{P}$
\end_inset

, based on the assumption that if multiple points in 
\begin_inset Formula $X$
\end_inset

 are 
\begin_inset Formula $\varepsilon$
\end_inset

-close to each other, we can eliminate all but one in our estimator.
 The VC Entropy is distribution-specific; it depends on the specific set
 of vectors 
\begin_inset Formula $q(\alpha)$
\end_inset

.
 In order to generate distribution-independant bounds, we establish the
 growth function 
\begin_inset Formula $G^{\Lambda}(\ell)$
\end_inset

 which describes the maximal value of 
\begin_inset Formula $H^{\Lambda}(\ell)$
\end_inset

 for any distribution given the set of estimators defined by 
\begin_inset Formula $Q(x:\ \alpha)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
G^{\Lambda}(\ell) & =\ln\sup_{x_{1},...,x_{\ell}}N^{\Lambda}(x_{1},...,x_{\ell})\\
H^{\Lambda}(\ell) & \le G^{\Lambda}(\ell)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
It can be shown that the following equations hold true with probability
 
\begin_inset Formula $(1-\eta)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\Psi & =4\frac{G^{\Lambda}\left(2\ell\right)-\ln\left(\eta/4\right)}{\ell}\\
R(\alpha) & \le R_{\text{emp}}(\alpha)+\frac{\Psi}{2}\left(1+\sqrt{1+\frac{4R_{\text{emp}}(\alpha)}{\Psi}}\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This equation describes the upper bound on the risk of estimators from the
 set 
\begin_inset Formula $Q(x:\ \alpha)$
\end_inset

 for 
\begin_inset Formula $\ell$
\end_inset

 observations, when the empirical risk is equal to 
\begin_inset Formula $R_{\text{emp }}(\alpha)$
\end_inset

.
 It can be further shown that the growth function is bounded by
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
G^{\Lambda} & \le h\left(\ln\frac{\ell}{h}+1\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $h$
\end_inset

 describes the VC Dimension of the estimator, defined as the maximum number
 of vectors that can be linearly separated by the estimator (in the case
 of binary estimators) or as the VC Dimension of the set of indicators 
\begin_inset Formula $I(x,\alpha,\beta)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
I(x,\alpha,\beta) & =\theta(Q(x:\ \alpha)-\beta),\ \alpha\in\Lambda,\ \beta\in(0,1)\\
\theta(x) & =\begin{cases}
0 & \quad\text{if}\ x<0,\\
1 & \quad\text{if}\ x\ge0\end{cases}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In other words the VC Dimension of a real-valued estimator is determined
 by the miximum number of vectors which can be enclosed by a region with
 radius 
\begin_inset Formula $1-\beta$
\end_inset

.
 In practical terms, this means that the VC Dimension is defined by the
 granularity of the estimator 
\begin_inset Formula $Q(x:\ \alpha)$
\end_inset

.
 Recall that PDF's are bounded by 
\begin_inset Formula $(0,1)$
\end_inset

; this means that each dimension of any loss vector 
\begin_inset Formula $q(\alpha_{n})$
\end_inset

 must be between 
\begin_inset Formula $(0,1)$
\end_inset

.
 Therefore the VC Dimension of an estimator is determined by the maximum
 number of regions with radius 
\begin_inset Formula $1-\beta$
\end_inset

 which can be defined on the interval 
\begin_inset Formula $(0,1)$
\end_inset

 in 
\begin_inset Formula $\ell$
\end_inset

 dimensions.
 In the case of SVM estimators, the value 
\begin_inset Formula $h$
\end_inset

 can be determined after optimizing for 
\begin_inset Formula $\beta$
\end_inset

 as the ratio of Support Vectors to 
\begin_inset Formula $\ell$
\end_inset

.
\end_layout

\begin_layout Subsection
Bounds of Convergence for TI Analysis
\end_layout

\begin_layout Standard
The growth function is used to characterize the worst-case performance for
 a specific type of estimator; we now show that the worst-case performance
 of an estimator capable of TI analysis is upper bounded by the growth function.
 Consider the PDF generated by a given estimator 
\begin_inset Formula $\varphi(x:\ \alpha_{n})$
\end_inset

 and two loss functions; the loss function 
\begin_inset Formula $Q(x:\ \alpha)$
\end_inset

 defined by the distance between 
\begin_inset Formula $\varphi(x:\ \alpha_{n})$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

, and the loss function 
\begin_inset Formula $Q_{TI}(x:\ \alpha)$
\end_inset

 defined by the TI distance between the same.
 It can easily be shown that 
\begin_inset Formula $Q(x:\ \alpha)$
\end_inset

 is a special case of 
\begin_inset Formula $Q_{TI}(x:\ \alpha)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
Q(x:\ \alpha) & =Q_{TI}(x:\ \alpha),\quad\mathbf{A}=I,\ \mathbf{b}=0\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Recall that TI estimators are generated from the integral of the distance
 between two sets under transformations.
 We can therefore construct the following inequality:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\|X,Y\| & \ge\int\|X,\mathbf{A}Y+\mathbf{b}\|d\mathbf{A}d\mathbf{b}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
It is clear therefore that the TI loss will be less than or equal to the
 distance loss, and that the growth function for a given closeness parameter
 will be less for the TI estimator than the distance estimator.
 
\end_layout

\begin_layout Subsection
Bayesian Model Averaging Ensemble Machine
\end_layout

\begin_layout Standard
We now develop a method of combining multiple estimators into a single learning
 machine.
 Our discussion of TI analysis has assumed that the distance metric is integrate
d over some set of dimensions 
\begin_inset Formula $D$
\end_inset

.
 In the case of high-dimensional datasets the computational demands of integrati
ng over set set of all dimensions in 
\begin_inset Formula $X$
\end_inset

 may outweigh the utility of doing so.
 We instead consider using multiple partitions of 
\begin_inset Formula $\bar{D}$
\end_inset

 to generate partial estimators.
 We can further extend the flexibility of the ensemble system by restricting
 each partial estimator to a subset of observations 
\begin_inset Formula $X^{D}\subseteq X$
\end_inset

.
 We therefore define a set of partial estimators:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi^{n}(x:\ X^{n},D^{n}) & \longmapsto\Pr(x|X^{n})\approx\Pr(x|X)\\
X^{n} & \subseteq X\\
D^{n} & \subseteq\bar{D}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
With associated risk bounds
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
In the case of Parzen Estimators we can set the empirical risk to 0, in
 the case of SVM estimators we can set the empirical risk to the minimal
 value of the optimization problem.
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\Psi^{n} & =4\frac{G^{\Lambda}\left(2|X^{n}|\right)-\ln\left(\eta/4\right)}{|X^{n}|}\\
R^{n}(\alpha) & \le R_{\text{emp}}^{n}(\alpha)+\frac{\Psi}{2}\left(1+\sqrt{1+\frac{4R_{\text{emp}}^{n}(\alpha)}{\Psi}}\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Consider the case of multiple partial estimators which makes estimates of
 
\begin_inset Formula $\Pr(x|X)$
\end_inset

 which we would like to combine.
 Consider the two resulting extimates of the probability of a given point
 
\begin_inset Formula $x$
\end_inset

.
 Using Bayesian Model Averaging (BMA) we can average the estimators weighted
 by their respective confidence interval
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Technically, they are being weighted by the probability that each estimator
 is accurate given 
\begin_inset Formula $X$
\end_inset

.
 Given a confidence interval, the relative probability that each estimate
 is accurate is determined by the confidence interval of the estimators.
 In this context, the concepts of risk and confidence interval are interchangabl
e.
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
If that footnote is correct, it's odd that we need to normalize this.
 And if we don't need to normalize it, how do we account for a thousand
 estimators with decent confidence interval being combined?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\Pr(x|X) & \approx\sum_{n}\Pr(x|\varphi^{n},X)\Pr(\varphi^{n}|X)\\
 & \approx\frac{1}{\sum_{n}1-R^{n}(\alpha)}\sum_{n}\varphi^{n}(x:\ X^{n},D^{n})(1-R^{n}(\alpha))\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Merging Estimators with Shared Dimensional Sets
\end_layout

\begin_layout Standard
This result allows us to create hybrid estimates based on a set of estimators
 regardless of the dimensional sets used by the estimators or the subsets
 of 
\begin_inset Formula $X$
\end_inset

 used by the estimators.
 We now consider a similar situation; combining the estimates of estimators
 based on the same dimensional set.
 In this context it is possible to to generate an estimator using the union
 of the observations from the two original estimators.
 This approach has a significant benefit over the BMA approach described
 above; it allows the hybrid estimator to generate estimates using the joint
 entropy of the observations, rather than the posterior probability of the
 marginal entropy of the two estimators.
 If we describe the entropy of two estimators as 
\begin_inset Formula $H(X^{1})$
\end_inset

 and 
\begin_inset Formula $H(X^{2})$
\end_inset

, the entropy of the union of these two subsets is lower bounded by the
 sum of the marginal entropies:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
H(X^{1}\cap X^{2}) & \ge H(X^{1})+H(X^{2})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Unfortunately we cannot simply combine the two subsets 
\begin_inset Formula $X^{1}$
\end_inset

 and 
\begin_inset Formula $X^{2}$
\end_inset

; doing so would violate the i.i.d.
 requirement of the PW estimator.
 To demonstrate this, consider a situation in which both subsets are drawn
 randomly from 
\begin_inset Formula $X$
\end_inset

 within some bounds 
\begin_inset Formula $(a^{1},b^{1})$
\end_inset

 and 
\begin_inset Formula $(a^{2},b^{2})$
\end_inset

 defined along some dimension of 
\begin_inset Formula $X$
\end_inset

 (ie we draw 
\begin_inset Formula $n$
\end_inset

 observations from 
\begin_inset Formula $X$
\end_inset

 from a specific time window).
 Within the context of each estimator the i.i.d.
 constraints are met within the defined bounds, however combining these
 two sets will only produce i.i.d.
 data if 
\begin_inset Formula $a^{1}=a^{2},\ b^{1}=b^{2}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Consider the requirement of i.i.d.
 data in PW estimators; the probability of a given point is determined by
 the sum of the kernel distance to each observation used by the estimator.
 More observations in a given region increases the probability estimate
 for any point near that region.
 Let us assume that the bounds of one estimator are contained in and smaller
 than the bounds of a second:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
a^{1} & <a^{2}\\
b^{1} & >b^{2}\\
|X^{1}| & =|X^{2}|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this case, a PW estimator would incorrectly generate elevated probability
 estimates in the region 
\begin_inset Formula $(a^{2},b^{2})$
\end_inset

 and lowered estimates in the compliment.
 We have already seen how SVM estimators achieve performance optimization
 by adjusting the weight 
\begin_inset Formula $\beta$
\end_inset

 given to each observation; we now propose a similar weighting mechanism
 
\begin_inset Formula $\rho(n)$
\end_inset

 which adjusts the influence of each point in the union of the two sets
 
\begin_inset Formula $X^{1}$
\end_inset

 and 
\begin_inset Formula $X^{2}$
\end_inset

 so that the estimates of a PW estimator based on the union of two i.i.d.
 data sets remain accurate estimates of the underlying PDF.
 We now show that the weighting mechanism must be based on the risk of the
 two estimators; given two estimators 
\begin_inset Formula $\varphi^{n}(x:\ X^{n},D^{i})$
\end_inset

 and 
\begin_inset Formula $\varphi^{m}(x:\ X^{m},D^{i})$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ X,D) & =\sum_{i=1}^{\ell}\frac{1}{\ell}K_{\gamma}(x,x_{i})\\
\varphi^{n,m}(x:\ X^{n}\cap X^{m}:D^{i}) & =\frac{\varphi^{n}(x:\ X^{n},D^{i})(1-R^{n}(\alpha))+\varphi^{n}(x:\ X^{n},D^{i})(1-R^{m}(\alpha))}{2-R^{n}(\alpha)-R^{m}(\alpha)}\\
 & =\frac{(1-R^{n}(\alpha))}{2-R^{n}(\alpha)-R^{m}(\alpha)}\sum_{x_{i}\in X^{n}}\frac{1}{|X^{n}|}K_{\gamma}(x,x_{i})+\frac{(1-R^{m}(\alpha))}{2-R^{n}(\alpha)-R^{m}(\alpha)}\sum_{x_{i}\in X^{m}}\frac{1}{|X^{m}|}K_{\gamma}(x,x_{i})\\
 & =\sum_{x_{i}\in X^{n}\cup X^{m}}\rho(x_{i}:\ \varphi^{n},\varphi^{m})\frac{1}{|X^{n}|+|X^{m}|}K_{\gamma}(x,x_{i})\\
\rho(x:\ \varphi^{n},\varphi^{m}) & =\begin{cases}
\frac{(1-R^{n}(\alpha))}{|X^{n}|\left(2-R^{n}(\alpha)-R^{m}(\alpha)\right)} & \quad x\in X^{n},x\notin X^{m}\\
\frac{(1-R^{m}(\alpha))}{|X^{m}|\left(2-R^{n}(\alpha)-R^{m}(\alpha)\right)} & \quad x\notin X^{n},x\in X^{m}\\
\frac{(1-R^{n}(\alpha))}{|X^{n}|\left(2-R^{n}(\alpha)-R^{m}(\alpha)\right)}+\frac{(1-R^{m}(\alpha))}{|X^{m}|\left(2-R^{n}(\alpha)-R^{m}(\alpha)\right)} & \quad x\in X^{n},x\in X^{m}\end{cases}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This can be extended to SVM estimators:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi^{n,m}(x:\ \beta,X) & =\sum_{x_{i}\in X^{n}\cup X^{m}}\beta_{i}\rho(x:\ \varphi^{n},\varphi^{m})K_{\gamma}(x,x_{i})\\
W(\beta:\ X) & \longmapsto\min_{\beta}\sum_{\begin{array}{c}
x_{i}\in X^{n}\cup X^{m}\\
x_{j}\in X^{n}\cup X^{m}\\
x_{k}\in X^{n}\cup X^{m}\end{array}}\beta_{j}\beta_{k}\rho(x_{j}:\ \varphi^{n},\varphi^{m})\rho(x_{k}:\ \varphi^{n},\varphi^{m})K_{\gamma}(x_{i},x_{j})K_{\gamma}(x_{i},x_{k})\\
 & \qquad\qquad\qquad-\sum_{\begin{array}{c}
x_{i}\in X^{n}\cup X^{m}\\
x_{j}\in X^{n}\cup X^{m}\end{array}}\frac{2\beta_{j}}{\ell}\rho(x_{j}:\ \varphi^{n},\varphi^{m})K_{\gamma}(x_{i},x_{j})+\lambda\Omega(\beta,X^{n}\cup X^{m})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
I don't think the second term is correct - work this through the equations.
 Also make sure the second term only needs one kernel term (note in SV section)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Note that both sets being combined must be i.i.d.; this means that single observati
ons in isolation cannot be added.
 The minimum number of points in a set required for the set to constitue
 i.i.d.
 data is determined by the number of dimensions in the dimensional set.
 The risk value 
\begin_inset Formula $R(\alpha)$
\end_inset

 is also a worst-case scenario, rather than an exact measure; the more points
 in a given subset.
 This can be accomodated by combining sets with sufficient observations
 that the contribution of the empirical risk is greater than the contribution
 of the growth function.
 In general terms, the more observations in the combined sets, the more
 accurate the weighting term 
\begin_inset Formula $\rho(\cdot)$
\end_inset

 will be.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Show a second iteration of this process
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Computational Considerations in Subset Selection
\end_layout

\begin_layout Standard
In general, computational demands will vary with different classes of estimators.
 TI estimators have far higher computational cost than point-based estimators.
 We assume that there exist more classes of estimators than are necessary
 to produce a given risk for a given set of observation, which leaves us
 with the task of selecting which estimators to apply to an estimation problem
 and how to assign observations to the selected estimators.
 To make these decisions, we develop a selection heuristic based on the
 costs and benefits of using a given estimator.
\end_layout

\begin_layout Standard
The basic approach we will describe is an iterative one; we first select
 an i.i.d.
 subset 
\begin_inset Formula $X'$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

, then add the subset to one or more estimators from the set of possible
 estimators based on some heuristic.
 At each iteration we update our selection heuristic based on the results
 of the previous addition of 
\begin_inset Formula $X'$
\end_inset

.
 This approach allows us to control both the set of estimators used and
 the observations assigned to each estimator; more importantly it allows
 us to do so based on the nature of the observations 
\begin_inset Formula $X$
\end_inset

.
 We will show that it is possible to build a heuristic which requires no
 additional computation, allowing us to simultaneously build estimators
 and optimize subsequent iterations.
\end_layout

\begin_layout Standard
The simplist portion of the heuristic to define is the cost of using an
 estimator.
 The computational complexity of estimators can generally be determined
 
\emph on
a priori
\emph default
 as a function of the number and dimensionality of the estimator.
 In some cases optimizations exist which are data-dependant; in these cases
 we will assume a worst-case scenario for additional observations.
 We define the cost function for a given estimator as 
\begin_inset Formula $c(n,X)$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 denotes the number of observations which will be added and 
\begin_inset Formula $X$
\end_inset

 denotes the observations used by the estimator.
\end_layout

\begin_layout Standard
The benefit portion of the heuristic can be easily constructed from previous
 work.
 For any estimator, the utility of the estimator is described by the risk
 associated with the estimator.
 In generating estiamtes our primary goal is to produce accurate estimates,
 and the risk function 
\begin_inset Formula $R(\alpha)$
\end_inset

 describes the lowest possible accuracy of a given estimator.
 It is clear therefore that the utility of adding observations to an estimator
 is determined by the change in the expected risk as a result of adding
 observations; estimators which we expect to gain a high reduction in risk
 should be preferred over estimators which we expect to gain a small reduction
 in risk.
 Recall that the maximal risk is determined by the number of observations
 used by the estimator, the empirical risk of the estimator, and the estimator's
 growth function; all three of these values are known for a given estimator
 withou additional computation
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The empirical risk for an SVM estimator is the minimal value of the optimization
 problem.
 The empirical risk of a Parzen Estimator can be assumed to be 0.
\end_layout

\end_inset

.
 We can therefore define the benefit function for a given estimator as 
\begin_inset Formula $b(n,X)$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 denotes the number of observations which will be added and 
\begin_inset Formula $X$
\end_inset

 denotes the observations used by the estimator:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
b(n,X) & =R(\alpha)-R(\alpha_{n})\\
R(\alpha_{n}) & =R_{\text{emp}}(\alpha)+n+\frac{\Psi_{n}}{2}\left(1+\sqrt{1+\frac{4R_{\text{emp}}(\alpha)+4n}{\Psi_{n}}}\right)\\
\Psi_{n} & =4\frac{G^{\Lambda}\left(2\left(|X|+n\right)\right)-\ln\left(\eta/4\right)}{|X|+n}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can now define our heuristic 
\begin_inset Formula $h(n,X)$
\end_inset

 as the difference of the benefit and the cost, scaled by some parameter
 
\begin_inset Formula $\lambda$
\end_inset

 which controls the trade-off between accuracy and performance:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
h(n,X) & =b(n,X)-\lambda c(n,X)\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Hierarchical Ensemble Estimators
\end_layout

\begin_layout Standard
The ensemble system presented above is formulated in the context of traditional
 point-based probability estimators (as opposed to set-based TI estimators).
 The equations also hold in the context of combining estimates from point-based
 and set-based estimators, in fact we can construct a system using multiple
 layers of hierarchical information - each level taking into account a larger
 set of observations.
 
\end_layout

\begin_layout Standard
Hierarchical systems allow us to implement data compression.
 While it is possible to compare all possible weighted subsets of a set
 of observations, in practice the computational demands of doing so quickly
 become prohibitive.
 Hierarchical systems allow us to break such an analysis into a set of simpler
 analyses, each of which implements a compression algorithm to eliminate
 redundant information.
 As a result, each layer in the hierarchy is able to operate in subset of
 the abstract space defined by the results of the lower layer.
\end_layout

\begin_layout Standard
Conceptually, our hierarchical system is defined at the lowest level by
 the vector observations 
\begin_inset Formula $X$
\end_inset

.
 Estimators defined in this hierarchical layer operate based on point-to-point
 comparisons, as is the case in standard SVM algorithms.
 The second layer within the hierarchy is defined by TI estimators; these
 estimators operate based on comparisons between different windows defined
 on 
\begin_inset Formula $X$
\end_inset

 by some windowing function 
\begin_inset Formula $\omega(\cdot)$
\end_inset

.
 The nature of the windowing function itself isn't important; the significant
 attribute of estimators in this layer are that they compare windows defined
 by 
\emph on
single points 
\emph default
and some windowing parameter 
\begin_inset Formula $\alpha$
\end_inset

.
 We can think of these two layers (the point-based and window-based estimators)
 as a pair; each operates based on points defined in 
\begin_inset Formula $X$
\end_inset

 - the distinguishing characteristic between them is that the former compares
 single points while the latter compares sets of points.
 To add additional layers to the hierarchy we introduce a new concept; abstract
 vectors.
 Abstract vectors allow us to compare subsets of 
\begin_inset Formula $X$
\end_inset

 using windows defined by 
\emph on
multiple 
\emph default
points.
\end_layout

\begin_layout Standard
We define an abstract vector (AV) as a representation 
\begin_inset Formula $y_{n}$
\end_inset

 of 
\begin_inset Formula $X$
\end_inset

 in some vicinity 
\begin_inset Formula $x_{n}$
\end_inset

 based on an estimator 
\begin_inset Formula $\varphi$
\end_inset

, which we assume to be an SVM estimator.
 The dimensionality of an AV is defined by the number of SV's in its estimator
 
\begin_inset Formula $\varphi$
\end_inset

; each SV is mapped to a single dimension in 
\begin_inset Formula $y_{n}$
\end_inset

.
 The value of a dimension mapped to a given SV 
\begin_inset Formula $x_{i}$
\end_inset

 is defined by the kernel distance between 
\begin_inset Formula $X$
\end_inset

 in the vicinity of 
\begin_inset Formula $x_{i}$
\end_inset

 and the SV 
\begin_inset Formula $x_{n}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
y_{n} & =[y_{n}^{1},...,y_{n}^{|X_{SV}|}]\\
X_{SV} & =[x_{i}:\ \beta_{i}>\epsilon]\\
y_{n}^{i} & =K(x_{n},x_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We treat SV's in different estimators as orthonormal; each SV in each estimator
 is mapped to a unique dimension of an AV.
 Because we can treat AV's as sparse datasets
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
A given AV can be represented by a pair of tuples defined by a SV index
 and a kernel distance.
 Since SV's are optimized to minimize overlap, we can assume that in most
 cases the number of SV's which are 
\begin_inset Formula $\epsilon$
\end_inset

-close to a given region of 
\begin_inset Formula $X$
\end_inset

 will be small.
 It is concievable that large numbers of high-dimensional observations in
 
\begin_inset Formula $X$
\end_inset

 could be compressed to a single SV:K tuple.
\end_layout

\end_inset

 the conversion of observations from 
\begin_inset Formula $\Omega^{X}\rightarrow\Omega^{Y}$
\end_inset

 vastly reduces the computation demands of subsequent hierarchical layers.
 This process can be seen as developing optimal codebooks for a given dataset
 and compressing observations using this codebook.
 The beauty of this approach is that lower layers not only provide optimal
 codebooks for higher layers, the process of discovering the optimal codebook
 simultaneously allows the lower layer to generate estimates.
 
\end_layout

\begin_layout Standard
We can now elaborate the nature of the third and fourth layers of our hierarchy;
 given a set of AV's constructed from estimators in layers one and two,
 we construct a new random variable 
\begin_inset Formula $Y$
\end_inset

 and treat it in the same manner as we did 
\begin_inset Formula $X$
\end_inset

.
 In this sense the hierarchical system is both hierarchical and iterative;
 each layer pair defines a new random variable which becomes the data on
 which subsequent layers are built.
 Between each pair of layers, the SV's act as a 'bridge' allowing us to
 translate patterns between layer pairs
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The regularizing term in this equation eliminates the requirement that the
 sum of an estimated AV's dimension be 1.
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\Pr(x|\ y) & \approx\frac{1}{\sum_{i}y^{i}}\sum_{x_{i}\in X_{SV}}y^{i}K(x,x_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this way, estimates generated in higher layers can be integrated into
 the BMA estimator by translating them into the abstract space of 
\begin_inset Formula $X$
\end_inset

.
 
\end_layout

\begin_layout Section
Prediction and Conditional Prediction
\end_layout

\begin_layout Standard
Now that we have developed a robust architecture for estimating the probability
 of a point, we turn our attention to the inverse problem; predicting observatio
ns which conform the the PDF, possibly under some set of conditions.
 In this sense, we are shifting our focus from probability fields to point
 estimates.
 Point estimates have many uses.
 They provide a concise summary of a PDF (i.e.
 polling data), they provide the ability to predict the most likely outcome
 of an estimate (i.e.
 weather forecasts), they allow investigation of subsets of the PDF (i.e.
 war games) etc.
 In general, when we refer to 'prediction', we will be referring to the
 generation of point sets, as opposed to 'estimation', by which we mean
 the generation of PDF's.
\end_layout

\begin_layout Standard
We will frame the prediction task as one of first constructing a suitable
 PDF, and then extracting a set of predictions.
 We will begin with the case of the PDF generated by the estimation technique
 developed in the previous sections and discuss a general method for generating
 predictions.
 We will then investigate various types of conditional PDF's and extend
 our prediction technique to these conditional PDF's.
\end_layout

\begin_layout Subsection
Maximum Probability Prediction
\end_layout

\begin_layout Standard
We begin by considering the case where we wish to determine the point in
 a PDF's abstract space 
\begin_inset Formula $\Omega$
\end_inset

 with the highest probability; we refer to this point as a prediction of
 
\begin_inset Formula $X$
\end_inset

.
 We consider the case where an estimate of the PDF 
\begin_inset Formula $\varphi(x:\ X)$
\end_inset

 exists, generated from some set of observations 
\begin_inset Formula $X=[x_{1},...,x_{\ell}]$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ X) & \longmapsto\Pr(x|X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
There are several approaches to determining the point 
\begin_inset Formula $\bar{x}=\max_{x}\varphi(x:\ X)$
\end_inset

; for instance we can solve for the roots of 
\begin_inset Formula $\nabla\varphi(x:\ X)$
\end_inset

 or we can use numerical optimization techniques such as gradient descent.
 Unfortunately there is no anylitical solution for solving the roots of
 
\begin_inset Formula $\varphi(x:\ X)$
\end_inset

 using the RBF kernel, and the computational complexity of solving for roots
 using other kernels makes this approach unattractive.
 Numerical optimization approaches are likewise computationally expensive.
 Rather than using these general approaches, we will develop an approach
 which takes advantage of the specific problem domain; PDF's generated by
 a summation of gaussian distributions with uniform width.
\end_layout

\begin_layout Standard
Let us re-frame the prediction task as the estimation of the conditional
 probability of 
\begin_inset Formula $x$
\end_inset

 given the set of observations 
\begin_inset Formula $X$
\end_inset

.
 In this context we can establish a regression function which describes
 the expected value of 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula $ $
\end_inset


\begin_inset Formula \begin{align*}
r(X) & =\int x\Pr(x|X)dx\\
 & =\sum_{i=1}^{\ell}x_{i}\varphi(x:\ X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The result of the regression function is not the point 
\begin_inset Formula $\bar{x}$
\end_inset

, but rather the 'center of mass' of the PDF.
 We can, however, modify the regression function to generate 
\begin_inset Formula $\bar{x}$
\end_inset

 by observing that for any two probabilties 
\begin_inset Formula $\Pr(x_{1}|X)$
\end_inset

 and 
\begin_inset Formula $\Pr(x_{2}|X)$
\end_inset

, the result of raising both probabilities to some power will produce a
 larger reduction in the lower probability:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\Pr(x_{1}|X) & >\Pr(x_{2}|X)\\
\Pr(x_{1}|X)-\Pr(x_{1}|X)^{n} & >\Pr(x_{2}|X)-\Pr(x_{2}|X)^{n},\ n>1\end{align*}

\end_inset


\end_layout

\begin_layout Standard
As 
\begin_inset Formula $n$
\end_inset

 increases, this difference will become more pronounced.
 If we consider this fact in the context of a PDF, we can see that raising
 a PDF to some power will tend to accentuate the peaks of the PDF and collapse
 the troughs; the center of mass of the PDF will be shifted towards the
 point in the PDF with the highest probability.
 We therefore propose the following modification of the regression function
 which we will refer to as the maximizing function 
\begin_inset Formula $m(\cdot)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
m(X:\ n) & =\frac{\int x\Pr(x|X)^{n}dx}{\int\Pr(x|X)dx}\\
 & =\frac{\sum_{i=1}^{\ell}x_{i}\left(\varphi(x:\ X)\right)^{n}}{\sum_{i=1}^{\ell}\left(\varphi(x:\ X\right)^{n}}\\
\lim_{n\rightarrow\infty}m(X:\ n) & =\max_{x}\Pr(x|X),\quad\text{if}\ \exists\sup_{x}\Pr(x|X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
As 
\begin_inset Formula $n$
\end_inset

 grows, the result of 
\begin_inset Formula $m(X:\ n)$
\end_inset

 will approach the point 
\begin_inset Formula $\bar{x}$
\end_inset

.
 In practice, we can pick a suitably large value of 
\begin_inset Formula $n$
\end_inset

 and check that result is sane by comparing the probability of 
\begin_inset Formula $m(X:\ n)$
\end_inset

 to the supremum of the probabilities of the support vectors.
 If 
\begin_inset Formula $m(X:\ n)<\sup_{x\in SV}\Pr(x|X)$
\end_inset

, we know that there are peaks in the PDF which have not been squelched
 by 
\begin_inset Formula $n$
\end_inset

.
\end_layout

\begin_layout Standard
In cases where the set of maximal elements contains multiple points this
 approach will not converge, and another approach will be necessary.
 One approach would be to add some randomness to the PDF, another would
 be to mask one of the maximal elements.
 Notice that for a given 
\begin_inset Formula $n$
\end_inset

 in which the solution has not converged, we can identify the SV's which
 constitute the maximal elements by evaluating each SV's individual contribution
:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\tilde{m}(x_{i}:\ n) & =\frac{x_{i}\left(\varphi(x:\ X)\right)^{n}}{\sum_{i=1}^{\ell}\left(\varphi(x:\ X)\right)^{n}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This may not actually tell you anything - the probability of the points
 may be near 0
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Generalized Conditional Prediction
\end_layout

\begin_layout Standard
We now consider conditional probability distributions.
 Conditional distributions describe the probability of points in 
\begin_inset Formula $\Omega$
\end_inset

 in a given context.
 We can view the estimation problem as the process of developing a conditional
 distribution where the context is the set of observations 
\begin_inset Formula $X$
\end_inset

.
 We now refine our treatment of the estimation problem to include both the
 set of observations 
\begin_inset Formula $X$
\end_inset

 and some context in which we are estimating.
 Our task is to restrict the estimation task to some subset 
\begin_inset Formula $\bar{\Omega}$
\end_inset

 of 
\begin_inset Formula $\Omega$
\end_inset

, where the difference between the two abstract spaces is a set of 
\emph on
given
\emph default
 values.
 This is to say we wish to estimate the probability of 
\begin_inset Formula $x\in\Omega$
\end_inset

, given some set of observations 
\begin_inset Formula $\bar{X}$
\end_inset

.
\end_layout

\begin_layout Standard
Because we can view the original estimation problem as a conditional estimation
 problem, the general approach is to add the conditional observations 
\begin_inset Formula $\bar{X}$
\end_inset

 to the original set of observations 
\begin_inset Formula $X$
\end_inset

 and proceed as before.
 We define the a conditional PDF estimate as:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\varphi(x:\ \bar{X},X) & \longmapsto\Pr(x|\bar{X})\\
\varphi(x:\ \bar{X},X) & =\varphi(x:\ \left[\bar{X}\cup X\right])\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this sense the 
\emph on
estimation
\emph default
 of conditional probability distributions is trivial.
 
\emph on
Prediction 
\emph default
in the context of conditional PDF's requires more consideration.
 Determining the point in a conditional PDF can be accomplished using the
 maximizing function 
\begin_inset Formula $m(\cdot)$
\end_inset

, however this does not really capture the nature of the conditional prediction
 task.
\end_layout

\begin_layout Standard
We consider the process of conditional prediction as determining a set of
 observations 
\begin_inset Formula $\hat{X}$
\end_inset

 whose conditional PDF 
\begin_inset Formula $\varphi(x:\ \hat{X},X)$
\end_inset

 conforms to some given conditional PDF 
\begin_inset Formula $\varphi(x:\ \bar{X},X)$
\end_inset

.
 This is to say our task is to predict a set of observations which we expect
 to produce a given outcome.
 The conditional prediction task can be seen as the inverse of the conditional
 estimation task; conditional estimation describes the PDF which matches
 some set of observations, while conditional prediction describes the set
 of observations whose conditional probability matches some given distribution.
 We describe the conformance between two conditional PDF's as the loss between
 them over the set of given observations.
 We call this the conditional risk 
\begin_inset Formula $R_{c}(\hat{X})$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
R_{c}(\hat{X}:\ \bar{X},X) & =\int\left(\varphi(x:\ \hat{X},X)-\varphi(x:\ \bar{X},X)\right)^{2}dx\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Given a conditional PDF 
\begin_inset Formula $\varphi(x:\ \bar{X},X)$
\end_inset

, there are likely multiple sets of predictions 
\begin_inset Formula $\hat{X}$
\end_inset

 which would produce a given conditional risk.
 In the same way that we restricted the prediction task to the point with
 the highest probability, we restrict the conditional prediction task to
 the set of points with the minimum entropy.
 In this sense, the prediction task is to find the least number of observations
 which would produce a given conditional risk.
 This corresponds to the SVM approach to PDF estimation; in both cases the
 solution will be unique, in both cases the solution can be found by solving
 a convex optimization problem, and in both cases we can controll the trade-off
 between the accuracy and sparsity of our solution with a regularizer parameteri
zed by 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Standard
We can now define the conditional prediction task: 
\emph on
conditional prediction is the task of determining the set of observations
 with the least entropy which minimize the conditional risk given 
\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\phi(\bar{X}:\ X) & \longmapsto\min_{\hat{X}}\left|R_{c}(\hat{X}:\ \bar{X},X)+\lambda H(\hat{X})\right|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The general definition of the conditional prediction task assumes that 
\begin_inset Formula $\hat{X}$
\end_inset

 can be any subset of 
\begin_inset Formula $\Omega$
\end_inset

, which in the case of a real-valued abstract space has cardinality 
\begin_inset Formula $2^{\aleph_{1}}$
\end_inset

.
 For this problem to be tractable we must restrict the domain of 
\begin_inset Formula $\hat{X}$
\end_inset

 to some finite subset of 
\begin_inset Formula $\Omega$
\end_inset

.
 We do so by restricting 
\begin_inset Formula $\hat{X}$
\end_inset

 to subsets of 
\begin_inset Formula $X$
\end_inset

, which has cardinality 
\begin_inset Formula $2^{\ell}$
\end_inset

.
 In this case we can define our optimization problem as analagous to the
 SVM optimization problem:
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
This is where the mistake is.
 We need to either search the full abstract space somehow, or generate points
 in the abstract space in some way.
\end_layout

\begin_layout Plain Layout
OK, so given a conditional pdf 
\begin_inset Formula $\varphi(x:\ \bar{X},X)$
\end_inset

 we can calculate the difference between it and the PDF of 
\begin_inset Formula $X$
\end_inset

: 
\begin_inset Formula $\delta(x:\ \bar{X},X)=\varphi(x:\ \bar{X},X)-\varphi(x:\ X)$
\end_inset

.
 The delta function describes the influence of each point in 
\begin_inset Formula $\Omega$
\end_inset

 on the conditional probability.
 The problem is to find the set of points 
\begin_inset Formula $\hat{X}$
\end_inset

 which best approximate 
\begin_inset Formula $\delta(\bar{X})$
\end_inset

.
 The catch here is that we're not looking for subsets of 
\begin_inset Formula $X$
\end_inset

, we're looking for 
\emph on
any
\emph default
 set of points.
 The delta function may not even be useful, as it's just a rearrangement
 of 
\begin_inset Formula $\phi(\cdot)$
\end_inset

 above.
 The utility of 
\begin_inset Formula $\delta(\bar{X})$
\end_inset

 is that we can calculate the maximal point easily.
 This would correspond to the first approach in Vapnik; hold the sparsity
 and minimize risk.
 In order to minimize both, we would need to integrate over a real-valued
 weighting function for each dimension.
 The only other approach I can see is to use transformations on the points
 in 
\begin_inset Formula $X$
\end_inset

, maybe we can find the optimal transformation sets? Could we use a modification
 of the regression function to produce transformation values? That's probably
 not going to work - the transformation would be in the exponent, and the
 entire estimator would need to be integrated.
 I think.
 It's worth thinking through - we should know that the number of points
 in 
\begin_inset Formula $X$
\end_inset

 is large enough to estimate 
\begin_inset Formula $\delta$
\end_inset

, and it's likely that points relevant to estimating 
\begin_inset Formula $X$
\end_inset

 would also be relevant to estimating conditional values of 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Plain Layout
How would we formulate the optimization problem? We want to both minimze
 the number of points and determine the transformation matrices.
 We would need to first optimize the transformation matrices to prevent
 overlap, and then optimize the results.
 Let's see if we can do the first part.
\end_layout

\begin_layout Plain Layout
Our objective is to find the optimal transformation matrix for each observation
 by minizing the conditional risk of the estimates from the transformed
 estimate.
 Let's do this in the context of 1-d observations.
 The transformation matrix for each point would be a scale parameter 
\begin_inset Formula $a$
\end_inset

 and a shift parameter 
\begin_inset Formula $b$
\end_inset

.
 We can call our transformed estimator
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
\varphi(x:\ \mathbf{A},\mathbf{b},X) & =\sum_{i=1}^{\ell}\beta_{i}K_{\gamma}(x,a_{i}x_{i}+b_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
The objective is to minimize the conditional risk of the transformed estimator
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
W(\mathbf{A},\mathbf{b}) & =\min\left|\sum_{x_{i}\in\bar{X}}\left(\sum_{j=1}^{\ell}\beta_{j}K_{\gamma}(x_{i},a_{j}x_{j}+b_{j})-\sum_{x_{j}\in\bar{X}\cup X}\beta_{j}K_{\gamma}(x_{i},x_{j})\right)^{2}\right|\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
There's not really much point in doing this - we're minimizing over a transforma
tion matrix and loosing any information in the original dataset.
 A simpler method would be to do what Vapnik suggests; fix the risk (which
 in this case means fixing the number of points we're estimating for) and
 minimize the sparseness.
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
W(\bar{x}) & =\min_{\bar{x}}\left|\sum_{x_{i}\in\bar{X}}\left(\sum_{\bar{x}_{j}}\frac{1}{|\bar{x}|}K_{\gamma}(x_{i},\bar{x}_{j})-\sum_{x_{j}\in\bar{X}\cup X}\beta_{j}K_{\gamma}(x_{i},x_{j})\right)^{2}\right|\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
We can make this feasible by substituting a new optimization variable 
\begin_inset Formula $\alpha$
\end_inset

 and using a univariate kernel function 
\begin_inset Formula $K_{\gamma}(x)$
\end_inset

 with a suitable cross-kernel function 
\begin_inset Formula $\mathcal{K}(\alpha,x)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
K_{\gamma}(x_{n},x_{m}) & =\alpha K_{\gamma}(x_{m})\\
\mathcal{K}(\alpha,x_{m}) & =x_{n}\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
In the case of the RBF kernel, we can define the functions as such
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
K_{\gamma}(x) & =e^{\frac{1}{\gamma}x^{2}}\\
\mathcal{K}(\alpha,x) & =x\pm\frac{1}{2}\sqrt{4x^{2}+4\log\alpha}\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
To restrict our solution to real numbers, we add the following constraint
 to the optimization problem:
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
\alpha_{i} & \le e^{x_{i}^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
We can no formulate the optimization problem:
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
W(\alpha) & =\min_{\alpha}\left|\int\left(\sum_{\alpha_{j}}\frac{\alpha_{j}}{|\alpha|}e^{\frac{1}{\gamma}x^{2}}-\varphi(x:\ \bar{X},X)\right)^{2}dx\right|\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula \[
\text{subject to}\quad\alpha_{i}\le e^{x_{i}^{2}}\]

\end_inset


\end_layout

\begin_layout Plain Layout
Let's consider the implications of using the delta function as the target
 of our optimization.
 If we're going to use the delta function we need to integrate over the
 entire abstract space, rather than just checking at 
\begin_inset Formula $\bar{X}$
\end_inset

:
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
W(\alpha) & =\min_{\alpha}\left|\int\left(\sum_{\alpha_{j}}\frac{\alpha_{j}}{|\alpha|}e^{\frac{1}{\gamma}x_{i}^{2}}-\delta(\bar{X},X)\right)^{2}dx\right|\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
This may be doable - let's try to work that integration through.
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
W(\alpha) & =\min_{\alpha}\left|\int\left(\sum_{\alpha_{j}}\frac{\alpha_{j}}{|\alpha|}e^{\frac{1}{\gamma}x^{2}}-\delta(x:\ \bar{X},X)\right)^{2}dx\right|\\
 & =\min_{\alpha}\left|\int\left(\sum_{\alpha_{i},\alpha_{j}}\frac{\alpha_{i}\alpha_{j}}{|\alpha|^{2}}e^{\frac{2}{\gamma}x^{2}}-\frac{2\alpha_{j}}{|\alpha|}\delta(x:\ \bar{X},X)+\delta(x:\ \bar{X},X)^{2}\right)dx\right|\\
 & =\min_{\alpha}\left|\int\sum_{\alpha_{i},\alpha_{j}}\frac{\alpha_{i}\alpha_{j}}{|\alpha|^{2}}e^{\frac{2}{\gamma}x^{2}}dx-\int\frac{2\alpha_{j}}{|\alpha|}\delta(x:\ \bar{X},X)dx\right|\\
 & =\min_{\alpha}\left|\sum_{\alpha_{i},\alpha_{j}}\frac{\alpha_{i}\alpha_{j}}{|\alpha|^{2}}\int e^{\frac{2}{\gamma}x^{2}}dx-\frac{2\alpha_{j}}{|\alpha|}\int\delta(x:\ \bar{X},X)dx\right|\\
 & =\min_{\alpha}\left|\sum_{\alpha_{i},\alpha_{j}}\frac{\alpha_{i}\alpha_{j}}{|\alpha|^{2}}\sqrt{\frac{\gamma\pi}{2}}-\int\frac{2\alpha_{j}}{|\alpha|}\delta(x:\ \bar{X},X)dx\right|\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
\int e^{\frac{2}{\gamma}x^{2}}dx & =\lim_{x\rightarrow\infty}\frac{\sqrt{\gamma}}{2}\sqrt{\frac{\pi}{2}}\text{erf}\left(\frac{\sqrt{2}x}{\sqrt{\gamma}}\right)-\lim_{x\rightarrow-\infty}\frac{\sqrt{\gamma}}{2}\sqrt{\frac{\pi}{2}}\text{erf}\left(\frac{\sqrt{2}x}{\sqrt{\gamma}}\right)\\
 & =\sqrt{\frac{\pi\gamma}{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
\int\delta(x:\ \bar{X},X)dx & =\int\varphi(x:\ \bar{X},X)-\varphi(x:\ X)\ dx\\
 & =\int\sum_{x_{i}\in\bar{X}\cup X}\beta_{i}K_{\gamma}(x,x_{i})-\sum_{x_{i}\in X}\beta_{i}K_{\gamma}(x,x_{i})\ dx\\
 & =\sum_{x_{i}\in\bar{X}\cup X}\beta_{i}\int K_{\gamma}(x,x_{i})dx-\sum_{x_{i}\in X}\beta_{i}\int K_{\gamma}(x,x_{i})dx\\
 & =\sum_{x_{i}\in\bar{X}\cup X}\beta_{i}-\sum_{x_{i}\in X}\beta_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
\int\delta(x:\ \bar{X},X)dx & =\int\left(\sum_{x_{i}\in\bar{X}\cup X}\beta_{i}K_{\gamma}(x,x_{i})-\sum_{x_{i}\in X}\beta_{i}K_{\gamma}(x,x_{i})\right)^{2}dx\\
 & =\int\sum_{\begin{array}{c}
x_{i}\in\bar{X}\cup X\\
x_{j}\in\bar{X}\cup X\end{array}}\beta_{i}\beta_{j}K_{\gamma}(x,x_{i})K_{\gamma}(x,x_{j})dx-\sum_{\begin{array}{c}
x_{i}\in\bar{X}\cup X\\
x_{j}\in X\end{array}}2\beta_{i}K_{\gamma}(x,x_{i})\beta_{j}K_{\gamma}(x,x_{j})+\sum_{\begin{array}{c}
x_{i}\in X\\
x_{j}\in X\end{array}}\beta_{i}\beta_{j}K_{\gamma}(x,x_{i})K_{\gamma}(x,x_{j})\ dx\\
 & =\sum_{\begin{array}{c}
x_{i}\in\bar{X}\cup X\\
x_{j}\in\bar{X}\cup X\end{array}}\beta_{i}\beta_{j}\int K_{\gamma}(x,x_{i})K_{\gamma}(x,x_{j})dx-\sum_{\begin{array}{c}
x_{i}\in\bar{X}\cup X\\
x_{j}\in X\end{array}}\beta_{i}\beta_{j}\int K_{\gamma}(x,x_{i})K_{\gamma}(x,x_{j})dx+\sum_{\begin{array}{c}
x_{i}\in X\\
x_{j}\in X\end{array}}\beta_{i}\beta_{j}\int K_{\gamma}(x,x_{i})K_{\gamma}(x,x_{j})\ dx\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
\int K_{\gamma}(x,x_{i})K_{\gamma}(x,x_{j})dx & =\int e^{-\frac{1}{\gamma}(x-x_{i})^{2}-\frac{1}{\gamma}(x-x_{j})^{2}}\\
 & =\lim_{x\rightarrow\infty}-\frac{1}{2}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}\sqrt{\frac{\gamma\pi}{2}}\text{erf}\left(\frac{x_{i}+x_{j}-2x}{\sqrt{2\gamma}}\right)-\lim_{x\rightarrow-\infty}-\frac{1}{2}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}\sqrt{\frac{\gamma\pi}{2}}\text{erf}\left(\frac{x_{i}+x_{j}-2x}{\sqrt{2\gamma}}\right)\\
 & =\sqrt{\frac{\gamma\pi}{2}}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
W(\alpha) & =\min_{\alpha}\left|\sum_{\alpha_{i},\alpha_{j}}\frac{\alpha_{i}\alpha_{j}}{|\alpha|^{2}}\sqrt{\frac{\gamma\pi}{2}}\right|\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
In this case, we would simply be estimating the points which would most
 likely cause 
\begin_inset Formula $\bar{X}$
\end_inset

.
 I'm not sure what to make of the fact that we can have negative values
 in here.
 I think that since the target functional is strictly positive, any negative
 points in 
\begin_inset Formula $\delta$
\end_inset

 would define a minimal risk value for the estimator; the best the estimator
 could do is zero out points in that area, which probably means they're
 ignored.
 We know that 
\begin_inset Formula $x\in\bar{X}$
\end_inset

 will always have higher probability in the conditional PDF - this may be
 meaningful.
\end_layout

\begin_layout Plain Layout
The next issue is how to control this thing.
 As it stands, it's almost certainly going to just cluster around 
\begin_inset Formula $\bar{X}$
\end_inset

.
 It would be nice to be able to nudge it towards predicting points farther
 out.
 The simplist thing is probably to use both the entropy of 
\begin_inset Formula $\alpha$
\end_inset

 and the entropy of 
\begin_inset Formula $\alpha$
\end_inset

 relative to 
\begin_inset Formula $\bar{X}$
\end_inset

; the first allows us to control sparseness, the second allows us to control
 how far from the conditional points we want to predict values for.
\end_layout

\begin_layout Plain Layout
Final issue; set-estimators.
 This one is a bitch - it fucks with the basic trick we used to get here.
 It's easy to solve for that variable, but if the distance metric is between
 sets, we end up in a situation where we need to generate a set with a given
 divergence from a known set, and nothing more to work from.
 We might, however, be able to integrate all these into a single optimization
 problem in which each LM contributes to the optimization of 
\begin_inset Formula $\alpha$
\end_inset

.
 In this case, we could just use the equation above.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
OK, new approach
\end_layout

\begin_layout Plain Layout
We're going to produce a set of estimates 
\begin_inset Formula $\hat{X}$
\end_inset

 which would produce a low conditional risk.
\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
W(\hat{X}) & =\min_{\hat{X}}\left|R_{c}(\hat{X}:\ \bar{X},X)\right|\\
 & =\min_{\hat{X}}\left|\int\left(\varphi(x:\ \hat{X},X)-\varphi(x:\ \bar{X},X)\right)^{2}\ dx\right|\\
 & =\min_{\hat{X}}\left|\int\left(\varphi(x:\ \hat{X},X)^{2}-2\varphi(x:\ \hat{X},X)\varphi(x:\ \bar{X},X)+\varphi(x:\ \bar{X},X)^{2}\right)dx\right|\\
 & =\min_{\hat{X}}\left|\int\varphi(x:\ \hat{X},X)^{2}dx-2\int\varphi(x:\ \hat{X},X)\varphi(x:\ \bar{X},X)dx+\int\varphi(x:\ \bar{X},X)^{2}dx\right|\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
\int\varphi(x:\ \bar{X},X)^{2}dx & =\int\sum_{i,j}\beta_{i}\beta_{j}K_{\gamma}(x,x_{i})K_{\gamma}(x,x_{j})\ dx\\
 & =\sum_{i,j}\beta_{i}\beta_{j}\int K_{\gamma}(x,x_{i})K_{\gamma}(x,x_{j})dx\\
 & =\sum_{i,j}\beta_{i}\beta_{j}\int e^{-\frac{1}{\gamma}(x-x_{i})^{2}-\frac{1}{\gamma}(x-x_{j})^{2}}dx\\
 & =\sum_{i,j}\beta_{i}\beta_{j}\left(\lim_{x\rightarrow\infty}-\frac{1}{2}\sqrt{\frac{\gamma\pi}{2}}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}\text{erf}\left(\frac{x_{i}+x_{j}-2x}{\sqrt{2g}}\right)-\lim_{x\rightarrow-\infty}-\frac{1}{2}\sqrt{\frac{\gamma\pi}{2}}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}\text{erf}\left(\frac{x_{i}+x_{j}-2x}{\sqrt{2g}}\right)\right)\\
 & =\sum_{i,j}\beta_{i}\beta_{j}\sqrt{\frac{\gamma\pi}{2}}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula \begin{align*}
W(\hat{X}) & =\min_{\hat{X}}\left|\sum_{i,j\in\hat{X}}\beta_{i}\beta_{j}\sqrt{\frac{\gamma\pi}{2}}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}-2\sum_{\begin{array}{c}
i\in\hat{X}\\
j\in\bar{X}\end{array}}\beta_{i}\beta_{j}\sqrt{\frac{\gamma\pi}{2}}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}+\sum_{i,j\in\bar{X}}\beta_{i}\beta_{j}\sqrt{\frac{\gamma\pi}{2}}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}\right|\\
 & =\min_{\hat{X}}\left|\sum_{i,j\in\hat{X}}\beta_{i}\beta_{j}\sqrt{\frac{\gamma\pi}{2}}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}-2\sum_{\begin{array}{c}
i\in\hat{X}\\
j\in\bar{X}\end{array}}\beta_{i}\beta_{j}\sqrt{\frac{\gamma\pi}{2}}e^{-\frac{1}{2\gamma}(x_{i}-x_{j})^{2}}\right|\end{align*}

\end_inset


\end_layout

\begin_layout Plain Layout
And this clarifies why the earlier approach doesn't work - you need to compare
 the optimization variables to each other in the exponent.
 We're back to not having a way to determine the points.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\phi(\bar{X}:\ X) & =\min_{\beta}\left|\sum_{x\in\hat{X}}\left(\varphi(x:\ \theta(\beta,X),X)-\varphi(x:\ \bar{X},X)\right)^{2}+\lambda H(\beta X)\right|\\
\theta(\beta_{i},x_{i}) & =\begin{cases}
x_{i} & \quad\beta_{i}=1\\
\emptyset & \quad\beta_{i}=0\end{cases}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \[
\text{subject to}\quad\beta_{i}\in[0,1]\]

\end_inset


\end_layout

\begin_layout Standard
Using the SVM estimator for 
\begin_inset Formula $\varphi(\cdot)$
\end_inset

, this reduces to
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\phi(\bar{X}:\ X) & =\min_{\beta}\left|\sum_{x\in\hat{X}}\left(\left(\beta_{i}\sum_{i=1}^{\ell}\beta_{i}^{n}K_{\gamma}(x,x_{i})\right)-\left(\sum_{x_{i}\in\bar{X}\cup X}\beta_{i}^{m}K_{\gamma}(x,x_{i})\right)\right)^{2}+\lambda H(\beta X)\right|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \[
\text{subject to}\quad\beta_{i}\in(0,1)\]

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\beta^{n}$
\end_inset

 is the optimal value for 
\begin_inset Formula $\varphi(x:\ X)$
\end_inset

 and 
\begin_inset Formula $\beta^{m}$
\end_inset

 is the optimal value for 
\begin_inset Formula $\varphi(x:\ \bar{X}\cup X)$
\end_inset

.
 In this case we interpret the optimal value of 
\begin_inset Formula $\beta_{i}$
\end_inset

 as the significance of 
\begin_inset Formula $x_{i}$
\end_inset

 and define the prediction as the set of observations 
\begin_inset Formula $\hat{X}=[x_{i}:\quad x_{i}\neq0]$
\end_inset

.
 In the estimation task, we referred to these observations as Support Vectors.
\end_layout

\begin_layout Subsection
Dimension-Conditional Prediction
\end_layout

\begin_layout Standard
The prediction function 
\begin_inset Formula $\phi(\cdot)$
\end_inset

 has a fundamental weakness; it's predictions are limited to subsets of
 the observations 
\begin_inset Formula $X$
\end_inset

.
 We now begin our investigation of methods of predicting novel sets.
 
\end_layout

\begin_layout Standard
In the generalized conditional prediction approach, we assumed that any
 subset of 
\begin_inset Formula $X$
\end_inset

 constituted a valid prediction.
 This assumption is based on the fact that we were selecting sets of points
 from a distribution.
 This assumption is not valid if our given observation constitues a set
 of values for a dimensional set 
\begin_inset Formula $\bar{D}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\bar{X} & =[\bar{x}]\\
\bar{x} & =[x^{n}:\ n\in\bar{D}]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this case, we are given a specific location in some subset 
\begin_inset Formula $\bar{\Omega}$
\end_inset

 of 
\begin_inset Formula $\Omega$
\end_inset

 and our task is to predict a set of values in the converse of 
\begin_inset Formula $\bar{D}$
\end_inset

 given 
\begin_inset Formula $\bar{X}$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Set-Conditional Prediction
\end_layout

\begin_layout Subsection
Field-Conditional Prediction
\end_layout

\begin_layout Standard

\lyxline

\end_layout

\begin_layout Standard
We need to develop a method of taking a probability density estimate and
 using it to predict values of 
\begin_inset Formula $X$
\end_inset

.
 In the most general sense, we can view this as finiding the point in 
\begin_inset Formula $X$
\end_inset

 with the highest probability.
 This isn't particularly useful, doing so will simply tell us which point
 in 
\begin_inset Formula $X$
\end_inset

 has the highest number of observations.
 The utility of prediction is derived from the ability to estimate the most
 probable value of 
\begin_inset Formula $x$
\end_inset

 given a partial set of values along the dimensions of 
\begin_inset Formula $x$
\end_inset

.
 This allows us to estimate the most likely value in some region of the
 abstract space.
 For instance, if our abstract space 
\begin_inset Formula $\Omega$
\end_inset

 contains a time dimension, we can make estimates of the most probable value
 of 
\begin_inset Formula $x$
\end_inset

 at a given time.
\end_layout

\begin_layout Standard
We can express this situation formally by establishing a set of conditional
 dimensions 
\begin_inset Formula $\bar{D}$
\end_inset

 and a set of prediction dimensions 
\begin_inset Formula $\hat{D}$
\end_inset

.
 Our task is to estimate the value of 
\begin_inset Formula $\hat{x}^{d},d\in\hat{D}$
\end_inset

 given the set of values 
\begin_inset Formula $\bar{x}^{d},d\in\bar{D}$
\end_inset

 using a prediction function 
\begin_inset Formula $\phi(\hat{x}:\ \bar{x},X)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\phi(\hat{x}:\ \bar{x},X) & \longmapsto\max_{\hat{x}}\Pr(\hat{x}|X)\approx\max_{\hat{x}}\varphi(\hat{x}:\ \hat{x},X)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We assume that predictions are made in the context of an ensemble system
 composed of multiple partial SVM estimators
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\max_{\hat{x}}\varphi(\hat{x}:\ \hat{x},X) & =\max_{x^{d},d\in\hat{D}}\left|\frac{1}{\sum_{n}1-R^{n}(\alpha)}\sum_{n}\varphi^{n}(x:\ X^{n},D^{n})(1-R^{n}(\alpha))\right|\\
 & =\max_{x^{d},d\in\hat{D}}\left|\frac{1}{\sum_{n}1-R^{n}(\alpha)}\sum_{n}(1-R^{n}(\alpha))\sum_{x_{i}\in X^{n}}\beta_{i}^{n}K(x,x_{i})\right|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can formulate this as a minimization problem by observing that probability
 estimates are upper-bounded by 1:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\max_{\hat{x}}\varphi(\hat{x}:\ \bar{x},X) & =\min_{x^{d},d\in\hat{D}}\left|1-\frac{1}{\sum_{n}1-R^{n}(\alpha)}\sum_{n}(1-R^{n}(\alpha))\sum_{x_{i}\in X^{n}}\beta_{i}^{n}K(x,x_{i})\right|\\
 & =\min_{x^{d},d\in\hat{D}}\left|-\frac{1}{\sum_{n}1-R^{n}(\alpha)}\sum_{n}(1-R^{n}(\alpha))\sum_{x_{i}\in X^{n}}\beta_{i}^{n}K(x,x_{i})\right|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We begin by considering a single estimator
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\max_{\hat{x}}\varphi(\hat{x}:\ \bar{x},X) & =\min_{x^{d},d\in\hat{D}}\left|-\sum_{x_{i}\in X}\beta_{i}K(x,x_{i})\right|\\
 & =\min_{x^{d},d\in\hat{D}}\left|-\sum_{x_{i}\in X}\beta_{i}\prod_{d\in\bar{D}}K(x^{d},x_{i}^{d})\prod_{d\in\hat{D}}K(x^{d},x_{i}^{d})\right|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Let us treat this as an optimization problem 
\begin_inset Formula $W(\sigma);\ \sigma=x^{i},i\in\hat{D}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\max_{\hat{x}}\varphi(\hat{x}:\ \bar{x},X) & =W(\sigma)\\
W(\sigma) & =\min_{\sigma}\left|-\sum_{x_{i}\in X}\beta_{i}\prod_{d\in\bar{D}}K(x^{d},x_{i}^{d})\prod_{\sigma}K(\sigma_{d},x_{i}^{d})\right|\\
 & =\min_{\sigma}\left|-\sum_{x_{i}\in X}\beta_{i}\prod_{d\in\bar{D}}K(x^{d},x_{i}^{d})\prod_{\sigma}e^{-\frac{1}{\gamma}\left\Vert \sigma_{d}-x_{i}^{d}\right\Vert }\right|\\
 & =\min_{\sigma}\left|-\sum_{x_{i}\in X}\beta_{i}\prod_{d\in\bar{D}}K(x^{d},x_{i}^{d})\ e^{-\frac{1}{\gamma}\sum_{\sigma}\left\Vert \sigma_{d}-x_{i}^{d}\right\Vert }\right|\\
 & =\min_{\sigma}\left|-\sum_{x_{i}\in X}\beta_{i}K_{i}\ e^{-\frac{1}{\gamma}\sum_{\sigma}\left\Vert \sigma_{d}-x_{i}^{d}\right\Vert }\right|\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can solve this by checking only at points where the gradient of 
\begin_inset Formula $\varphi$
\end_inset

 is 0:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\max_{\hat{x}}\varphi(\hat{x}:\ \bar{x},X) & =\max\psi\\
\psi & =\left[x_{i}:\ \sum\nabla\varphi(x)=0\right]\\
\nabla\varphi(x) & =\left[\frac{\partial\varphi}{\partial x^{n}}\sum_{i=1}^{|X|}\beta_{i}K(x^{n},x_{i}^{n})\prod_{d\in\bar{D}\vee n}K(x^{d},x_{i}^{d}),\ n\in\hat{D}\right]\\
\nabla\varphi(x)_{n} & =\frac{\partial\varphi}{\partial x^{n}}\sum_{i=1}^{|X|}\beta_{i}K(x^{n},x_{i}^{n})\prod_{d\in\bar{D}\vee n}K(x^{d},x_{i}^{d})\\
 & =\sum_{i=1}^{|X|}\beta_{i}\left(\frac{\partial\varphi}{\partial x^{n}}K(x^{n},x_{i}^{n})\right)\prod_{d\in\bar{D}\vee n}K(x_{j}^{d},x_{i}^{d})\\
\frac{\partial\varphi}{\partial x^{n}}K(x^{n},x_{i}^{n}) & =\frac{\partial\varphi}{\partial x^{n}}e^{-\frac{1}{\gamma}(x^{n}-x_{i}^{n})^{2}}\\
 & =e^{-\frac{1}{\gamma}(x^{n}-x_{i}^{n})^{2}}\frac{\partial\varphi}{\partial x^{n}}-\frac{1}{\gamma}(x^{n}-x_{i}^{n})^{2}\\
 & =e^{-\frac{1}{\gamma}(x^{n}-x_{i}^{n})^{2}}\frac{\partial\varphi}{\partial x^{n}}-\left(\frac{1}{\gamma}\left(x^{n}\right)^{2}-\frac{2}{\gamma}x^{n}x_{i}^{n}+\left(x_{i}^{n}\right)^{2}\right)\\
 & =e^{-\frac{1}{\gamma}(x^{n}-x_{i}^{n})^{2}}\left(-\frac{2}{\gamma}x^{n}+\frac{2}{\gamma}x_{i}^{n}\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
0 & =\sum_{i=1}^{|X|}\beta_{i}\left(\frac{\partial\varphi}{\partial x^{n}}K(x^{n},x_{i}^{n})\right)\prod_{d\in\bar{D}\vee n}K(x_{j},x_{i}^{d})\\
 & =\sum_{i=1}^{|X|}e^{-\frac{1}{\gamma}(x^{n}-x_{i}^{n})^{2}}\left(-\frac{2}{\gamma}x^{n}+\frac{2}{\gamma}x_{i}^{n}\right)\prod_{d\in\bar{D}\vee n}\beta_{i}K(x_{j},x_{i}^{d})\\
 & =\sum_{i=1}^{|X|}e^{-\frac{1}{\gamma}(x^{n}-x_{i}^{n})^{2}}\left(x_{i}^{n}-x^{n}\right)f(\bar{D},X,x_{i},\beta_{i})\\
 & =\sum_{i=1}^{|X|}e^{-\frac{1}{\gamma}\left(x^{n}\right)^{2}+\frac{2}{\gamma}x^{n}x_{i}^{n}-\frac{1}{\gamma}\left(x_{i}^{n}\right)^{2}}\left(x_{i}^{n}-x^{n}\right)f(\bar{D},X,x_{i},\beta_{i})\\
 & =e^{-\frac{1}{\gamma}\left(x^{n}\right)^{2}}\sum_{i=1}^{|X|}e^{\frac{2}{\gamma}x^{n}x_{i}^{n}-\frac{1}{\gamma}\left(x_{i}^{n}\right)^{2}}\left(x_{i}^{n}-x^{n}\right)f(\bar{D},X,x_{i},\beta_{i})\\
 & =\sum_{i=1}^{|X|}e^{\frac{2}{\gamma}x^{n}x_{i}^{n}-\frac{1}{\gamma}\left(x_{i}^{n}\right)^{2}}\left(x_{i}^{n}-x^{n}\right)f(\bar{D},X,x_{i},\beta_{i})\\
 & =\sum_{i=1}^{|X|}e^{a_{i}x^{n}+b_{i}}\left(x_{i}^{n}-x^{n}\right)f(\bar{D},X,x_{i},\beta_{i})\\
 & =\sum_{i=1}^{|X|}x_{i}^{n}f(\bar{D},X,x_{i},\beta_{i})e^{a_{i}x^{n}+b_{i}}-x^{n}f(\bar{D},X,x_{i},\beta_{i})e^{a_{i}x^{n}+b_{i}}\\
 & =\sum_{i=1}^{|X|}c_{i}e^{a_{i}x^{n}+b_{i}}-d_{i}x^{n}e^{a_{i}x^{n}+b_{i}}\\
 & =\sum_{i=1}^{|X|}c_{i}e^{a_{i}x^{n}+b_{i}}-\sum_{j=1}^{|X|}d_{j}x^{n}e^{a_{j}x^{n}+b_{j}}\\
\sum_{i=1}^{|X|}d_{i}x^{n}e^{a_{i}x^{n}+b_{i}} & =\sum_{j=1}^{|X|}c_{j}e^{a_{j}x^{n}+b_{j}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
OK - bottom line is that you're not going to find the set of saddle points
 in the estimator using gradients - another approach is going to be required.
 One thing we do know is that the estimator is the sum of bumps defined
 by the observations, and for a given estimator each of these bumps is the
 same width.
 It may be possible to at least optimize the gradient descent algorithm
 based on this knowledge.
\end_layout

\begin_layout Plain Layout
Let's go back to the observation that we're dealing with estimating the
 point with the highest probablity *given* some dimensional values of that
 point.
 We may be able to assume that points near our assumed values will also
 be near the point with the maximum probability.
 We know for sure that the point with the maximum probability will be *at*
 the assumed values, so the only question really is if that point is near
 other observations.
 I think this is a legitimate assumption, since the probability is based
 on those very points.
 It's possible that the maximal point is *between* observations, but it
 can't be more than 1 standard deviation (in the bump size) away from any
 of them (I think).
\end_layout

\begin_layout Plain Layout
This should allow us to restrict our search space, or at least determine
 a sane starting point; we simply take the point(s) closest to our assumed
 values and use them as starting points for the gradient ascent.
 This limits the number of starting points to the dimensionality of the
 input space minus the dimensionality of the assumed points (that many points
 could be equidistant in different dimensions).
 In reality this number will probably be lower.
\end_layout

\begin_layout Plain Layout
Another observation is that not only can we restrict our attention to observatio
ns near our assumed value, we can gain some information from the distance
 matrix between those observations; possibly enough to bypass the search.
 Using the distance between points, we may be able to simply average the
 points.
 In fact, this approach may work without the threshold, and since we already
 have the distance matrix this would be a fairly simple process.
\end_layout

\begin_layout Plain Layout
Let's think about this.
 Start with the Parzen case, where each bump is the same size and width;
 the only thing which increases probability over the basic bump is points
 being near to each other.
 Let's look at two points first.
 In this case we know that the maximum will either be at the center of the
 bumps or midway between them.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
f(x,y) & =e^{-(a-x)^{2}}+e^{-(a-y)^{2}}\\
\frac{\partial f}{\partial a}(x,y) & =\frac{\partial f}{\partial a}e^{-a^{2}+2ax-x^{2}}+\frac{\partial f}{\partial a}e^{-a^{2}+2ay-y^{2}}\\
 & =\left(-2a+2x\right)e^{-(a-x)^{2}}+\left(-2a+2y\right)e^{-(a-y)^{2}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
(2a-2y)e^{-(a-y)^{2}} & =(2x-2a)e^{-(a-x)^{2}}\\
 & =(2x-2a)e^{-a^{2}+2ax-x^{2}}\\
(2a-2y)e^{-a^{2}}e^{2ay}e^{-y^{2}} & =(2x-2a)e^{-a^{2}}e^{2ax}e^{-x^{2}}\\
(a-y)e^{2ay}e^{-y^{2}} & =(x-a)e^{2ax}e^{-x^{2}}\\
2ay-y^{2}+\log(a-y) & =2ax-x^{2}+\log(x-a)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
OK, so you can't even determine 2 points analytically.
 We're back in the realm of guesses and estimates.
 The only useful result of all this is that we can calculate the probability
 of the SV's near the assumed values and then use the gradient function
 to determine local maxima in the neighborhoods of SV's.
 It would still be nice to be able to cluster the SV's in some way, so that
 we don't end up finding a single local minima based on multiple gradient
 ascents from SV's on the same 'mound'.
 A simple approach would be to take the SV with the maximal probability,
 then eliminate from our search any points within some neighborhood of the
 SV, then repeat.
 It would be helpful to have a way to determine that threshold value (along
 with the value for which SV's are close enough to warrant searching).
\end_layout

\begin_layout Plain Layout
Let's see if we can establish that value from a different angle
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\end{align*}

\end_inset

Given two points 
\begin_inset Formula $x_{1}=0$
\end_inset

 and 
\begin_inset Formula $x_{2}=1$
\end_inset

 which define a Parzen estimator defined by the kernel 
\begin_inset Formula $K_{\gamma}(\cdot,\cdot)$
\end_inset

 defined by the width parameter 
\begin_inset Formula $\gamma$
\end_inset

.
\begin_inset Formula $ $
\end_inset

 If 
\begin_inset Formula $\gamma$
\end_inset

 is small, the PDF will consist of two prominant 'bumps', but as 
\begin_inset Formula $\gamma$
\end_inset

 grows, the two bumps will merge into a single one.
 Our task is to determine the value of 
\begin_inset Formula $\gamma$
\end_inset

 at which the point of maximum probability shifts from being at 
\begin_inset Formula $x_{1}$
\end_inset

 and 
\begin_inset Formula $x_{2}$
\end_inset

 to being at 
\begin_inset Formula $\bar{x}=.5$
\end_inset

.
 The value of 
\begin_inset Formula $\gamma$
\end_inset

 at which this takes place is the value of gamma for which the probability
 of these three points is equal:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\end{align*}

\end_inset


\begin_inset Formula \begin{align*}
K(x_{1},x_{1})+K(x_{1},x_{2}) & \le K(\bar{x},x_{1})+K(\bar{x},x_{2})\\
K(x_{2},x_{1})+K(x_{2},x_{2}) & \le K(\bar{x},x_{1})+K(\bar{x},x_{2})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Let's start with the basics and see if we can build up.
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
f(x) & =e^{-(x-a)^{2}}\\
\frac{\partial f}{\partial x}(x) & =\left(2a-2x\right)e^{-(x-a)^{2}}\\
\max_{x}f(x) & \in\left[x:\ \frac{\partial f}{\partial x}(x)=0\right]\\
0 & =\frac{\partial f}{\partial x}(x)\\
 & =\left(2a-2x\right)e^{-(x-a)^{2}}\\
2xe^{-(x-a)^{2}} & =2ae^{-(x-a)^{2}}\\
\log2x+\log e^{-(x-a)^{2}} & =\log2a+\log e^{-(x-a)^{2}}\\
\log2x-(x-a)^{2} & =\log2a-(x-a)^{2}\\
x & =a\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
f(x) & =e^{-(x)^{2}}+e^{-(x-a)^{2}}\\
\frac{\partial f}{\partial x}(x) & =-2xe^{-(x)^{2}}+\left(2a-2x\right)e^{-(x-a)^{2}}\\
\max_{x}f(x) & \in\left[x:\ \frac{\partial f}{\partial x}(x)=0\right]\\
0 & =\frac{\partial f}{\partial x}(x)\\
 & =-2xe^{-(x)^{2}}+\left(2a-2x\right)e^{-(x-a)^{2}}\\
2xe^{-(x)^{2}} & =\left(2a-2x\right)e^{-(x-a)^{2}}\\
2xe^{-x^{2}} & =\left(2a-2x\right)e^{-x^{2}+2ax-a^{2}}\\
2xe^{-x^{2}} & =\left(2a-2x\right)e^{-x^{2}}e^{2ax}e^{-a^{2}}\\
2x & =\left(2a-2x\right)e^{-x^{2}}e^{2ax}e^{-a^{2}}\\
1 & =\frac{2a-2x}{2x}e^{-x^{2}}e^{2ax}e^{-a^{2}}\\
1 & =\left(\frac{a}{x}-1\right)e^{-x^{2}}e^{2ax}e^{-a^{2}}\\
0 & =2ax-x^{2}-a^{2}+\log\left(\frac{a}{x}-1\right)\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Let's see what happens if we use the multi quadratic kernel:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
f(x,y) & =\beta\frac{1}{\frac{\left(x-y\right)^{2}}{\gamma^{2}}+a}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This has the nice property of being always positive (no bounds required).
 Let's see if we can determine the multiplier
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\int f(x,y)dx & =-\frac{\gamma}{\sqrt{a}}\tan^{-1}\left(\frac{y-x}{\gamma\sqrt{a}}\right)\\
\int_{-\infty}^{\infty}f(x,y)dx & =\lim_{x\rightarrow\infty}-\frac{\gamma}{\sqrt{a}}\tan^{-1}\left(\frac{y-x}{\gamma\sqrt{a}}\right)-\lim_{x\rightarrow-\infty}-\frac{\gamma}{\sqrt{a}}\tan^{-1}\left(\frac{y-x}{\gamma\sqrt{a}}\right)\\
 & =-\frac{\gamma}{\sqrt{a}}\cdot\frac{-\pi}{2}+\frac{\gamma}{\sqrt{a}}\cdot\frac{\pi}{2}\\
 & =\frac{\gamma\pi}{\sqrt{a}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Which means we can use the following as a kernel function (and be sure that
 it will produce PDF's):
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
K(x,y) & =\frac{\gamma\pi}{\frac{\sqrt{a}(x-y)^{2}}{\gamma^{2}}+a\sqrt{a}}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Another potential kernel is the Triweight (we can also use the quartic by
 dropping the exponent and changing the multiplier to 15/16).
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
K(x,y) & =\frac{35}{32\gamma}\left(1-\left(\frac{x-y}{\gamma}\right)^{2}\right)^{3}\mathbf{1}_{|x-y|\le1}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The triweight has a much faster cutoff than either of the others, and the
 explicit bounds are both good (gives us a clean way to ignore observations)
 and bad (could leave us with no estimate of PDF in a region).
 The benefit of the triweight is that it's a polynomial, which means that
 the roots of summations over it will be anylitically determinable.
 The multiquadratic has some nice features - lots of control over the width
 and falloff, but having the variables in the denominator like that makes
 me nervous.
\end_layout

\begin_layout Standard

\lyxline

\end_layout

\begin_layout Standard
Vapnik has an interesting approach, the last thing he shows is how to calculate
 the conditional probability 
\begin_inset Formula $\Pr(y|x)$
\end_inset

, and throws out a regression equation in the process:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
r(x) & =\int y\Pr(y|x)dy\\
 & =\int yK_{\gamma}(y,y_{i})dy\\
 & =\sum_{i=1}^{\ell}y_{i}\beta_{i}K_{\gamma}(x,x_{i})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Keep in mind that the last two of these are based on a different optimization
 problem.
 If we treat 
\begin_inset Formula $x,y$
\end_inset

 as dimensional sets, we should be able to define them for any observation,
 and we should be able to calculate the conditional probabilities of any
 dimensional subset given an observation in a disjoint subset.
 Let's start by developing this possiblity; we want to estimate the probability
 of a vector 
\begin_inset Formula $x^{D_{e}}$
\end_inset

 given a vector 
\begin_inset Formula $x^{D_{g}}$
\end_inset

, 
\begin_inset Formula $\Pr(x^{D_{g}}|x^{D_{e}})$
\end_inset

 based on a set of observations 
\begin_inset Formula $X=[x_{1}^{D},...,x_{\ell}^{D}];\ D_{c}\subset D,\ D_{g}\subset D,\ D_{c}\cup D_{g}=\emptyset$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This first line is the only one I'm not sure about.
 It should work - it's as though you're only taking the probability of the
 subset D of x.
 From there I think the rest should hold.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\Pr(x^{D}) & =\sum_{i=1}^{\ell}\beta_{i}\prod_{d\in D}K_{\gamma}(x^{d},x_{i}^{d})\\
\Pr(x^{d}|x^{D_{g}}) & =\Pr(x^{d\cup D_{g}})\Pr(x^{D_{g}})\\
 & =\sum_{i=1}^{\ell}\beta_{i}\prod_{\bar{d}\in d\cup D_{g}}K_{\gamma}(x^{\bar{d}},x_{i}^{\bar{d}})\ \sum_{i=1}^{\ell}\beta_{i}\prod_{\bar{d}\in D_{g}}K_{\gamma}(x^{\bar{d}},x_{i}^{\bar{d}})\\
 & =\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\beta_{i}\beta_{j}\prod_{d\in D_{c}\cup D_{g}}K_{\gamma}(x^{d},x_{i}^{d})\prod_{d\in D_{g}}K_{\gamma}(x^{d},x_{i}^{d})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can now adopt the earlier equation to our new approach:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
r(x^{D_{g}}) & =\int x^{D_{c}}\Pr(x^{D_{c}}|x^{D_{g}})dx^{D_{c}}\\
r(x^{D_{g}}) & =\left[\int x^{d}\Pr(x^{d}|x^{D_{g}})dx^{d}:\quad d\in D_{g}\right]\\
\int x^{d}\Pr(x^{d}|X^{D_{g}})dx^{d} & =\int x^{d}\Pr(x^{d\cup D_{g}})\Pr(x^{D_{g}})\\
 & =\int x^{d}\left[\sum_{\begin{array}{c}
i=1\\
j=1\end{array}}^{\ell}\beta_{i}\beta_{j}\prod_{\bar{d}\in d\uplus D_{g}}K_{\gamma}(x^{\bar{d}},x_{i}^{\bar{d}})\prod_{\bar{d}\in D_{g}}K_{\gamma}(x^{\bar{d}},x_{i}^{\bar{d}})\right]dx^{d}\\
 & =\sum_{i=1}^{\ell}x_{i}^{d}\sum_{\begin{array}{c}
j=1\\
k=1\end{array}}^{\ell}\beta_{i}\beta_{j}\prod_{\bar{d}\in d\cup D_{g}}K_{\gamma}(x_{i}^{\bar{d}},x_{j}^{\bar{d}})\prod_{\bar{d}\in D_{g}}K_{\gamma}(x_{i}^{\bar{d}},x_{k}^{\bar{d}})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The regression function can be seen as determining the center of mass of
 the PDF.
 Unfortunately, in the case of a distribution with local maxima, the regression
 function will essentially average those maxima, producing a result which
 may not have a probability value near the maximum; consider for example
 a distribution with two 'humps'; the regression function will produce a
 value in the middle of the humps - a value which is not itself near the
 maxima of either hump.
\end_layout

\begin_layout Standard
We can find the global maximum by effectively increasing the contrast of
 the regression function, increasing the peaks and decreasing the troughs.
 We can do this by raising the probability estimate at a given point to
 some power 
\begin_inset Formula $a$
\end_inset

; points with higher probabilities will be reduced less than points with
 lower probabilities:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
r(x^{D_{g}}:\ a) & =\sqrt[a]{\int x^{D_{c}}\left(\Pr(x^{D_{c}}|x^{D_{g}})\right)^{a}dx^{D_{c}}}\end{align*}

\end_inset


\end_layout

\begin_layout Section
Motivated Decision-Making
\end_layout

\begin_layout Standard
Up to this point, our discussion has focused on data analysis.
 We now turn our attention to volitive systems; systems which have the ability
 influence the the probability distribution 
\begin_inset Formula $\mathcal{P}$
\end_inset

 of the observation space 
\begin_inset Formula $\Omega$
\end_inset

, and in which certain types of local probability distributions are preferred
 over others.
 
\end_layout

\begin_layout Subsection
Setting of the Motivation Problem
\end_layout

\begin_layout Standard
We assume that such a system influences 
\begin_inset Formula $\mathcal{P}$
\end_inset

 through actions which are discrete and quantifiable.
 Each action is therefore described as a vector 
\begin_inset Formula $\vec{a}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\vec{a}_{i} & =\left[a_{i}^{1},...,a_{i}^{\pi}\right]\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In order to correlate actions with changes in 
\begin_inset Formula $\mathcal{P}$
\end_inset

, we treat each action as an observation of 
\begin_inset Formula $X$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
X & =(\Omega,\mathcal{F},\mathcal{P})\\
\Omega & \in\mathbb{R}^{d+\pi}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In this case, our observations are of dimension 
\begin_inset Formula $d+\pi$
\end_inset

, the first 
\begin_inset Formula $d$
\end_inset

 dimensions describe the system's 'environment' and the last 
\begin_inset Formula $\pi$
\end_inset

 describe the system's 'actions'.
 In order to choose between possible actions, the system must prefer certain
 states of the input space 
\begin_inset Formula $\Omega$
\end_inset

 over others.
 We describe these preferred states as the system's using a function 
\begin_inset Formula $m_{\theta}(x:\ X)$
\end_inset

 we'll refer to as the motivator with control parameter 
\begin_inset Formula $\theta$
\end_inset

.
 The for a given dimension of 
\begin_inset Formula $\Omega$
\end_inset

, the motivator function returns a positive real value describing the system's
 preference for the state 
\begin_inset Formula $x^{i}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
m_{\theta}(x^{i}:\ X) & \in\mathbb{R}^{+}\\
m_{\theta}(x^{i}:\ X)>1 & \longmapsto\text{preferred state}\\
m_{\theta}(x^{i}:\ X)<1 & \longmapsto\text{discouraged state}\\
m_{\theta}(x^{i}:\ X)=1 & \longmapsto\text{no preference}\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Estimating Outcomes in the Incentive Space
\end_layout

\begin_layout Standard
Motivated decision making therefore is a two-stage process; the system must
 first create an estimate of the relationship between actions and observations
 of 
\begin_inset Formula $X$
\end_inset

 in the context of those actions and then estimate action values which maximize
 the probability of preferred states given the action's context.
 The first stage can be accomplished using the TI analysis already developed.
 The second stage requries that we apply the motivator to the estimation
 task in a specific context.
 In this case our estimates are no longer probability estimates; instead
 they describe the probability that a given action will correspond to preferred
 states in the context of the action.
 We refer to these estimates as as pints in the incentive space 
\begin_inset Formula $I$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
\bar{\varphi}(x:\ X) & \longmapsto\Pr(x|I)\\
\bar{\varphi}(x:\ X) & =\prod_{\upsilon=1}^{d}\sum_{i=1}^{|X|}m_{\theta}(x_{i}^{\nu}:\ X)\beta_{i}\ \bar{K}_{\gamma}(x^{\nu},x_{i}^{\nu},X^{\nu})\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Still not sure if the first one makes any sense
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this sense, the motivator function behaves like a weighting mechanism,
 increasing the incentive value of high-probability actions which correspond
 to preferred states and decreasing the incentive value of high-probability
 actions which correspond with discouraged states.
 Estimating values in the incentive space allows us to select actions whose
 influence is well understood to produce the desired states of 
\begin_inset Formula $X$
\end_inset

 in a given context.
 Choosing actions is reduced to determining the supremum of the action dimension
s of the incentive space in a given context.
\end_layout

\begin_layout Subsection
Incentivized Path Search
\end_layout

\begin_layout Standard
\begin_inset Marginal
status open

\begin_layout Plain Layout
This is a work in progress - ignore
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Because estimates can now influence the underlying data, generating estimates
 is no longer a simple matter of perception; the choice of a specific action
 will influence the probability space, making the next estimation task different
 from the last.
 Furthermore, in the context of complex actions made up of multiple vectors,
 the choice of a single action may modify the incentive space independant
 of any influence from the motivator function.
 Maximizing the expected incentive value in a deterministic and computationally
 feasible manner, in the context of these facts, would be of great utility
 and is left as an exercise for the reader.
\end_layout

\begin_layout Standard
We will instead focus on iterative approaches.
 We can create a set of actions 
\begin_inset Formula $A$
\end_inset

 through an iterative process.
 We first define a selector function 
\begin_inset Formula $s(X:\ D)$
\end_inset

 which selects a value of 
\begin_inset Formula $a_{i}$
\end_inset

 from the incentive space defined by 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $D$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
s(X:\ D) & \longmapsto a_{i}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
At each step 
\begin_inset Formula $i+1$
\end_inset

 in the iteration we select a context 
\begin_inset Formula $D$
\end_inset

 and select an action 
\begin_inset Formula $a_{i}=s(X_{i}:\ D)$
\end_inset

 from action dimensions for 
\begin_inset Formula $X_{i}$
\end_inset

 in the given context.
 We then append 
\begin_inset Formula $a_{i}$
\end_inset

 to 
\begin_inset Formula $X_{i}$
\end_inset

 and repeat the process, this time with the set 
\begin_inset Formula $X_{i+1}=X_{i}\cup a_{i}$
\end_inset

.
 
\end_layout

\begin_layout Standard
If 
\begin_inset Formula $s(X:\ D)$
\end_inset

 simply selects the supremum of the action dimensions of 
\begin_inset Formula $X$
\end_inset

, the system is susceptible to local minima.
 Various techniques exist to address this situation; we will simply assume
 that 
\begin_inset Formula $s(X:\ D)$
\end_inset

 is indeterminant and involves some element of randomness.
 This means that at each iteration, multiple values of 
\begin_inset Formula $a_{i}$
\end_inset

are possible.
 We can therefore think of our iterative approach as a tree search, with
 multiple search paths originating from each iteration.
 This leaves us with not simply a set of action paths 
\begin_inset Formula $A$
\end_inset

, but a set of paths, each one tracing a different route from trunk to tip,
 and each one producing different estimates of the expected incentives in
 different contexts.
 This presents yet another problem of choice; which branch of actions should
 actually be taken?
\end_layout

\begin_layout Standard
To address this problem, we define an ethics function 
\begin_inset Formula $e(A_{i})$
\end_inset

.
 The ethics function establishes different weights for different incentive
 values in different contexts:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
e(A_{i}) & \in\mathbb{R}^{+}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We refer to this function as 'ethical' because it makes trade-offs between
 the probability of satisfying the systems motivation and the context to
 which that probability refers; is it better to buy a new car today or save
 my money to buy a house in a year? 
\end_layout

\begin_layout Standard
We can now generate a set of actions which is likely to produce known results
 which will satisfy the system's motivational parameters by selecting the
 supremum of the results of our path search:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
A & =\sup_{A}e(A_{1})\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Curiosity
\end_layout

\begin_layout Standard
Without the ability to take action, a system's accuracy and efficiency for
 a given dataset is limited by the algorithm it uses and the computational
 resources available to it.
 Systems with the ability to influence the observations from which they
 make estimates, on the other hand, can take advantage of this ability to
 increase both their accuracy and performance.
 Before we can discuss the mechanisims by which this is possible, we must
 first develop a more rigorous understanding of these two objectives; accuracy
 and efficiency.
\end_layout

\begin_layout Standard
Any probabilistic system's estimation accuracy is limited by the amount
 of uncertainty inherent in the data it processes and the nature of the
 algorithm used to generate estimates.
 The rate of convergence in such a system's estimations and the 'true' probabili
ty distribution has an upper bound determined by its VC dimension; within
 these bounds a system's estimation accuracy improves as the number of observati
ons increases.
 In the context of TI data, it is possible to think of each neighborhood
 in a TI dimension as an independant estimation problem, thus an observation
 of 
\begin_inset Formula $X$
\end_inset

 will not increase the system's accuracy in a given neighborhood if the
 observation 
\begin_inset Formula $x$
\end_inset

 is not in that neighborhood.
 The task of improving a volitive system's accuracy in a given context therefore
 can be framed as the task of obtaining observations of that context.
 
\end_layout

\begin_layout Standard
The efficiency of the system we've developed is primarily controlled by
 the number of Support Vectors.
 For a given dataset and predictive accuracy, as the number of SV's decreases,
 the efficiency of the system increases.
 The efficiency of non-volitive systems is limited by the results of the
 optimization problem for 
\begin_inset Formula $\beta$
\end_inset

.
 In the context of a volitive system, decreasing the number of SV's can
 be accomplished by obtaining observations higher-order structure.
 Higher-order structure refers to the ability of a multi-layered system
 to break multiple high-entropy observations down into shared combinations
 of lower-entropy observations in such a way that the entropy of combined
 observations is lower than the entropy of the original observations.
 The result of such a process is the ability to eliminate portions of the
 original dataset as redundant, reducing the number of required SV's to
 match the accuracy of the system prior to observing the higher-order structure.
\end_layout

\begin_layout Standard
Working within the context of the existing architecture, our task is to
 develop a metric which can be used as a motivational parameter to drive
 these two behaviors.
 If we assume that the sampling rate of the system is independant of the
 action choices which are made, both behaviors can be reduced to minimizing
 the ratio of observations to support vectors.
 Increasing the system's accuracy can be viewed as determining a suitable
 support vector (or vectors) to represent behavior of 
\begin_inset Formula $X$
\end_inset

 in a given context (allowing the redundant observations to be discarded)
 and promoting the discovery of higher-order structure inherently increases
 this ratio by the same mechanism.
 We can therefore create a curious system by establishing the performance
 metric 
\begin_inset Formula $p(\beta,X)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
p_{|X|}(\beta,X) & =\frac{|X|}{|\beta|-|\beta_{0}|}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\beta_{0}=[\beta_{i}|\ \beta_{i}=0]$
\end_inset

.
 Motivating the system towards curiosity simply entails establishing a motivator
 which prefers positive values of the second-derivative of 
\begin_inset Formula $p(\beta,X)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula \begin{align*}
m(x_{i}^{p}:\ X) & =e^{\lambda\frac{d^{2}}{di^{2}}p_{i}(\beta,X)}\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Using the second derivative motivates the system to prefer increases in
 the average change in 
\begin_inset Formula $p(\beta,X)$
\end_inset

.
 The value of 
\begin_inset Formula $p(\beta,X)$
\end_inset

 is likely non-stationary, so establishing a motivational parameter for
 set values isn't helpful.
 The first-derivative of 
\begin_inset Formula $p(\beta,X)$
\end_inset

 establishes the expected change in the system's comprehension, however
 this rate will vary with different datasets and would need to be determined
 for each dataset.
 The second-derivative however describes the rate at which the system's
 comprehension is increasing or decreasing, and is more likely to be tied
 to the system's actions than to the underlying data.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
This needs some housekeeping - explain values of p at each observation and
 work the work 'curiosity' and 'comprehension' into the earlier paragraphs
\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Eunite Competition Data
\end_layout

\begin_layout Subsection
Santa Fe Data
\end_layout

\begin_layout Subsection
CATS Benchmark Data
\end_layout

\begin_layout Subsection
Results Summary
\end_layout

\begin_layout Section
Further Research
\end_layout

\begin_layout Subsection
Data Pre-Processing
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function using mean and sd to put most training points between
 .1 and .9
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% upper and lower bounds on 
\backslash
(
\backslash
int 
\backslash
Delta 
\backslash
) as non-stationary data detections mechanism
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% logistic function from delta using mean and sd in same way if data non-station
ary
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

% iterative integration process until stationary data found ?
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Performance
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
Eat it, bitches
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nocite{Moreno03}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
bibliographystyle{plain}
\end_layout

\end_inset

 
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "Research/research.bib"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
